#!/usr/bin/env python

"""
Version 0.1
no speed buffs, no extra functionality, no optimisation
Basic Pipeline
for multi core desktops, not clusters

Issues:
1) if this script is killed, the rosetta will continue
2) Theseus can hang, this is killed after a timeout
3) is clustered by all atom RMSD to make fine clusters (option to cluster by CA only i included)
4) ASU content is the number of search models placed by MrBUMP. -- need to set this manually


================================

so...
Only data passed between different stages are the pdb files, so each stage could just start from a directory of files

"""

import os
import sys

# Test for environment variables

if not "CCP4" in sorted(os.environ.keys()):
    raise RuntimeError('CCP4 not found')

# Add the ample python folder to the PYTHONPATH
sys.path.append(os.path.join(os.environ["CCP4"], "share", "ample", "python"))

# python imports
import argparse
import clusterize
import glob
import logging
import re
import shutil
import time

# Our imports
import ample_options
import ample_util
#import fasta_parser
import mrbump_ensemble
import rosetta_model
import run_spicker
import SCWRL_edit
import split_models, nmr
import truncateedit_MAX


#-------------------------------------------
# get command line options
#-------------------------------------------

parser = argparse.ArgumentParser(description='Structure solution by abinitio modelling', prefix_chars="-")

#
# Rosetta arguments
#
parser.add_argument('-rosetta_db', metavar='ROSETTA_database', type=str, nargs=1,
                   help='path for Rosetta database')

parser.add_argument('-ROSETTA_cluster', metavar='path to Rosettas cluster', type=str, nargs=1,
                   help='location of rosetta cluster')

parser.add_argument('-ROSETTA', metavar='ROSETTA_path', type=str, nargs=1,
                   help='path for Rosetta AbinitioRelax')

parser.add_argument('-rosetta_dir', metavar='Rosetta_dir', type=str, nargs=1,
                   help='the Rosetta install directory')

# Rosetta fragments
parser.add_argument('-rosetta_fragments_exe', metavar='path to make_fragments.pl', type=str, nargs=1,
                   help='location of make_fragments.pl')

parser.add_argument('-make_frags', metavar='bool to make fragments', type=str, nargs=1,
                   help='Bool, True to make non homologous framents, False to import fragments')

parser.add_argument('-frags3mers', metavar='frags3mers', type=str, nargs=1,
                   help='path of imported 3mers')

parser.add_argument('-frags9mers', metavar='frags9mers', type=str, nargs=1,
                   help='path of imported 9mers')

parser.add_argument('-fasta', metavar='fasta_file', type=str, nargs=1,
                   help='protein fasta file. (required)')

parser.add_argument('-name', metavar='priotein name', type=str, nargs=1,
                   help='name of protien in the format ABCD ')

parser.add_argument('-nproc', metavar='NoProcessors', type=int, nargs=1,
                   help='number of processers (default 1)')


parser.add_argument('-run_dir', metavar='run_directory', type=str, nargs=1,
                   help='directory to put files (default current dir)')


parser.add_argument('-scwrl_exe', metavar='path to scwrl', type=str, nargs=1,
                   help='pathway to SCWRL exe')


parser.add_argument('-LGA', metavar='path_to_LGA dir', type=str, nargs=1,
                   help='pathway to LGA folder (not the exe) will use the \'lga\' executable')


parser.add_argument('-maxcluster_exe', metavar='Maxcluster exe', type=str, nargs=1,
                   help='Maxcluster exe')

parser.add_argument('-spicker_exe', metavar='boolean', type=str, nargs=1,
                   help='path to spicker executable')

parser.add_argument('-theseus_exe', metavar='Theseus exe (required)', type=str, nargs=1,
                   help='path to theseus executable')

parser.add_argument('-phenix_exe', metavar='PHENIX.ensembler exe', type=str, nargs=1,
                   help='path to phenix executable')

parser.add_argument('-mtz', metavar='MTZ in', type=str, nargs=1,
                   help='MTZ in')


parser.add_argument('-models_dir', metavar='folder of decoys', type=str, nargs=1,
                   help='folder of decoys')

parser.add_argument('-make_models', metavar='Do the modelling', type=str, nargs=1,
                   help='run rosetta modeling, set to False to import pre-made models (required if making models locally default True)')


# # FLAGS
parser.add_argument('-F', metavar='flag for F', type=str, nargs=1,
                   help='Flag for F')

parser.add_argument('-SIGF', metavar='flag for SIGF', type=str, nargs=1,
                   help='Flag for SIGF')

parser.add_argument('-FREE', metavar='flag for FREE', type=str, nargs=1,
                   help='Flag for FREE')

parser.add_argument('-cluster_submit', metavar='using a cluster', type=str, nargs=1,
                   help='will submit jobs to a cluster')

parser.add_argument('-cluster_qtype', metavar='cluster_qtype', type=str, nargs=1,
                   help='cluster submission queue type')

parser.add_argument('-allatom', metavar='allatom', type=str, nargs=1,
                   help='allatom')

parser.add_argument('-domain_all_chains_pdb', metavar='domain_all_chains_pdb', type=str, nargs=1,
                   help='fixed input to mr bump')

parser.add_argument('-domain_all_chains_fasta', metavar='domain_all_chains_fasta', type=str, nargs=1,
                   help='fasta for all ASU')

parser.add_argument('-domain_termini_distance', metavar='termini', type=str, nargs=1,
                   help='distance between termini for insert domains')


parser.add_argument('-nmodels', metavar='no models', type=int, nargs=1,
                   help='number of models to make (1000)')


parser.add_argument('-CC', metavar='rad of gyration', type=str, nargs=1,
                   help='radius of gyration reweight ')

parser.add_argument('-ensembles', metavar='enembles', type=str, nargs=1,
                   help='path to ensembles')

parser.add_argument('-num_clusters', metavar='num clusters to sample', type=int, nargs=1,
                   help='number of clusters to sample')


parser.add_argument('-percent', metavar='no clus to sample', type=str, nargs=1,
                   help='percent interval for truncation')

parser.add_argument('-ASU', metavar='no in ASU', type=int, nargs=1,
                   help='no in ASU')

# jmht - not used?
#parser.add_argument('-FreeLunch', metavar='free lunch', type=int, nargs=1,
#                   help='true to use free lunch, false to not use it')

parser.add_argument('-usehoms', metavar='nohoms rosetta flag', type=str, nargs=1,
                   help='True =use nhomologs, False= dont use them ')

parser.add_argument('-improve_template', metavar='improve template', type=str, nargs=1,
                   help='give a template to imrove - NMR, homolog ')

parser.add_argument('-debug', metavar='debug option', type=str, nargs=1,
                   help='debug option ')

parser.add_argument('-ensembler', metavar='ensembler', type=str, nargs=1,
                   help='ensembling')

parser.add_argument('-early_terminate', metavar='early_terminate', type=str, nargs=1,
                     help='terminate early if a success')

parser.add_argument('-shelx_cycles', metavar='shelx_cycles', type=str, nargs=1,
                     help='number of shelx sycles')


parser.add_argument('-molreponly', metavar='molreponly', type=str, nargs=1,
                   help='molreponly')

parser.add_argument('-phaseronly', metavar='phaseronly', type=str, nargs=1,
                   help='phaseronly')

parser.add_argument('-import_cluster', metavar='import_cluster', type=str, nargs=1,
                   help='import_cluster')

parser.add_argument('-old_shelx', metavar='old_shelx', type=str, nargs=1,
                   help='old_shelx')

parser.add_argument('-top_model_only', metavar='top_model_only', type=str, nargs=1,
                   help='top_model_only')

parser.add_argument('-use_scwrl', metavar='USE_SCWRL', type=str, nargs=1,
                   help='if using scwrl true if not false')

parser.add_argument('-alignment_file', metavar='alignment_file', type=str, nargs=1,
                   help='alignment between homolog and target fastas')

parser.add_argument('-NMR_model_in', metavar='NMR_model_in', type=str, nargs=1,
                   help='use nmr input')

parser.add_argument('-NMR_process', metavar='NMR_process', type=str, nargs=1,

                   help='number of times to process the models')

parser.add_argument('-NMR_remodel_fasta', metavar='-NMR_remodel_fasta', type=str, nargs=1,
                   help='fasta_for_remodeling')

parser.add_argument('-NMR_Truncate_only', metavar='-NMR_Truncate_only', type=str, nargs=1,
                   help='do no remodelling only truncate the NMR')

parser.add_argument('-use_buccaneer', metavar='use_buccaneer', type=str, nargs=1,
                   help='True to use Buccaneer')

parser.add_argument('-buccaneer_cycles', metavar='buccaneer_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-arpwarp_cycles', metavar='arpwarp_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-use_arpwarp', metavar='-use_arpwarp', type=str, nargs=1,
                   help='True to use arpwarp ')

parser.add_argument('-use_shelxe', metavar='-use_shelxe', type=str, nargs=1,
                   help='True to use shelx ')

parser.add_argument('-mr_keys', metavar='-mr_keys', type=str, nargs=1,
                   help='mrbump keywords ')

#
# jmht - set the default values here
#
# name -> pdb_code
# SCRWL -> SCWRL_path
# maxcluster_exe -> maxcluster_exe_path
parser.set_defaults( 
                    alignment_file = '',
                    allatom = "true",
                    arpwarp_cycles = 10,
                    ASU = 0,
                    buccaneer_cycles = 5,
                    CC = '',
                    cluster_submit = "false",
                    cluster_qtype = "SGE",
                    import_cluster = "false",
                    debug = "false",
                    domain_all_chains_pdb = None,
                    domain_all_chains_fasta = None,
                    domain_termini_distance = 0,
                    ensembles = None,
                    ensembler = "false",
                    fasta = None,
                    frags3mers=None,
                    frags9mers=None,
                    improve_template = None,
                    maxcluster_exe = None,
                    make_frags = "true",
                    make_models = "true",
                    models_dir = None,
                    molreponly = "false",
                    mtz = None,
                    name = 'ABCD',
                    num_clusters = 1,
                    nmodels = 1000,
                    NMR_model_in = '',
                    NMR_process = None,
                    NMR_remodel_fasta = '',
                    NMR_Truncate_only = "false",
                    nproc = 1,
                    shelx_cycles = 15,
                    old_shelx = "false",
                    percent = 5,
                    phaseronly = "false",
                    phenix_exe = None,
                    rosetta_cluster = None,
                    rosetta_db  = None,
                    rosetta_fragments_exe = None,
                    rosetta_path =  None,
                    run_dir = os.getcwd(),
                    scwrl_exe = None,
                    spicker_exe = None,
                    usehoms = "false",
                    use_arpwarp = "true",
                    use_buccaneer = "true",
                    use_scwrl = 'false',
                    use_shelxe = "false",
                    top_model_only = "false",
                    theseus_exe = None,
                    early_terminate = "true"
                    )


# convert args to dictionary
args = parser.parse_args()

# get MRBUMP keywords direct
MRkeys = []
# print sys.argv
keycount = 0
while keycount < len(sys.argv):
            # print sys.argv[keycount] ,  keycount
    if sys.argv[keycount] == "-mr_keys":
        MRkeys.append(sys.argv[keycount + 1])
    keycount += 1


# Populate the options from the parser
amopt = ample_options.AmpleOptions( )
amopt.populate( args )

# Now set MRKeys
amopt.d['mr_keys'] = MRkeys
#jmht - fix these
FIXED_INPUT = True
pdb_code = amopt.d['name']
amopt.d['pdb_code'] = amopt.d['name']

# qtype always use uppercase
amopt.d['cluster_qtype'] = amopt.d['cluster_qtype'].upper()

# Make a work directory and go there - this way all output goes into this directory
if not os.path.exists(amopt.d['run_dir']):
    print 'You need to give a run directory'
    sys.exit()

work_dir = ample_util.make_workdir( amopt.d['run_dir'] )
amopt.d['work_dir'] = work_dir
os.chdir(work_dir)

# Set up logging
logger = ample_util.setup_logging()

# Check if importing ensembles
amopt.d['import_ensembles'] = False
if os.path.isdir( str(amopt.d['ensembles']) ):
    
    amopt.d['import_ensembles'] = True
    logger.info("Found directory with ensemble files: {0}\nSetting make_models to False".format( amopt.d['ensembles'] ) )
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False
    
# Check if importing models
amopt.d['import_models'] = False
if amopt.d['models_dir']:
    
    if not os.path.isdir( amopt.d['models_dir'] ):
        msg = "Trying to import models from the directory:\n{0}\nBut it is not a directory!".format(amopt.d['models_dir'])
        logger.critical(msg)
        raise RuntimeError,msg
    
    amopt.d['import_models'] = True
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False

else:
    amopt.d['models_dir'] = amopt.d['work_dir'] + os.sep + "models"

# Check models and ensembles
if amopt.d['import_ensembles'] and amopt.d['import_models']:
        msg = "Cannot import both models and ensembles!"
        logger.critical(msg)
        raise RuntimeError, msg


# NMR Checks
NMR_PROTOCOL = False
if os.path.isfile( amopt.d['NMR_model_in'] ):
    NMR_model_in = amopt.d['NMR_model_in']
    NMR_PROTOCOL = True
    ROSETTA_CM = amopt.d['rosetta_dir'] + '/rosetta_source/bin/mr_protocols.default.linuxgccrelease'
    if not os.path.exists(ROSETTA_CM) :
        print ' cant find ' + ROSETTA_CM + ' check path names'
        sys.exit()
    
if os.path.isfile( amopt.d['NMR_remodel_fasta'] ):
    NMR_remodel_fasta = amopt.d['NMR_remodel_fasta']
else:
    NMR_remodel_fasta = amopt.d['fasta']

if not  NMR_PROTOCOL :
    if amopt.d['import_models'] and not os.path.exists(amopt.d['models_dir']):
        logger.warn('you have chosen to import models, but path does not exist, looking for ensembles : ' + amopt.d['models_dir'])
        if not os.path.exists(amopt.d['ensembles']):
            logger.warn('you have chosen to import ensembles, but path does not exist : ' + amopt.d['ensembles'])
            if amopt.d['import_cluster']:
                logger.info('you are importing a cluster of models from :' + amopt.d['import_cluster'])
                if not os.path.exists(amopt.d['import_cluster']):
                    logger.critical('you have chosen to import a cluster, the clusterpath does not exist')
                    sys.exit()

# Missing domains
if amopt.d['domain_all_chains_pdb'] or amopt.d['domain_all_chains_fasta']:
    #jmht - fix
    amopt.d['fasta'] = amopt.d['domain_all_chains_fasta']
    logger.info('Processing missing domain\n')
    if not ( os.path.exists(amopt.d['domain_all_chains_fasta']) or \
         os.path.exists(amopt.d['domain_all_chains_pdb']) ):
        logger.critical(' Cannot find either domain_all_chain_fasta: {0}\n or domain_all_chain_pdb: {1}'.format(amopt.d['domain_all_chains_fasta'],amopt.d['domain_all_chains_pdb'] )  )
        sys.exit(1)
elif not ( os.path.exists(str(amopt.d['fasta'])) or \
         os.path.exists(str(amopt.d['NMR_remodel_fasta'])) ):
    logger.critical('You need to give the path for the fasta')
    sys.exit(1)
    
# jmht - dont' think this used/needed
#logger.debug('Parsing FASTA file')
#outfasta = os.path.join(work_dir, pdb_code + '_.fasta')
#fasta_parser.parse_fasta(FASTA, outfasta)
#FASTA = outfasta

# Create the rosetta modeller - this runs all the checks required
if amopt.d['make_models'] or amopt.d['make_frags']:  # only need Rosetta if making models
    rosetta_modeller = rosetta_model.RosettaModel()
    rosetta_modeller.set_from_amopt( amopt )

#
# Variables for running MRBUMP
if amopt.d['ASU'] > 0:
    amopt.d['ASU'] = 'NMASU ' + str( amopt.d['ASU']  )
else:
    amopt.d['ASU'] = str(amopt.d['ASU'])

FIXED_INTERVALS = True
if amopt.d['percent'] != 5:
    FIXED_INTERVALS = False
percent = amopt.d['percent']

amopt.d['mrbump_programs'] = ' molrep phaser '
if not amopt.d['molreponly'] and not amopt.d['phaseronly']:
    amopt.d['mrbump_programs'] = ' molrep phaser '
if amopt.d['molreponly']:
    amopt.d['mrbump_programs'] = ' molrep  '
if amopt.d['phaseronly']:
    amopt.d['mrbump_programs'] = ' phaser  '
if amopt.d['molreponly'] and amopt.d['phaseronly']:
        logger.critical('you say you want molrep only AND phaser only, choose one or both')
        sys.exit()    

#
#Check we can find all the required programs
#
if amopt.d['ensembler']:
    logger.info('You are using Phenix ensembler')
    amopt.d['phenix_exe'] = ample_util.check_for_exe('phenix', amopt.d['phenix_exe'])
else:
    amopt.d['theseus_exe'] = ample_util.check_for_exe('theseus', amopt.d['theseus_exe'])

if amopt.d['use_scwrl']:
    amopt.d['scwrl_exe'] = ample_util.check_for_exe('Scwrl4', amopt.d['scwrl_exe'] )

amopt.d['maxcluster_exe'] = ample_util.check_for_exe('maxcluster', amopt.d['maxcluster_exe'])
amopt.d['spicker_exe'] = ample_util.check_for_exe('spicker', amopt.d['spicker_exe'])


# Check if we need to get any of the flags from the MTZ file
if not amopt.d['F'] or not amopt.d['SIGF'] or not amopt.d['FREE']:
    
    if not os.path.isfile( amopt.d['mtz'] ):
        logger.critical("Cannot find MTZ file: {0}".format( amopt.d['mtz'] ) )
        sys.exit(1)
    try:
        t_flag_F, t_flag_SIGF, t_flag_FREE = ample_util.get_mtz_flags( amopt.d['mtz'] )
    except KeyError,e:
        logger.critical("Error generating flags from MTZ file: {0}\n{1}\nYou may need to run the CCP4 uniqueify on the MTZ file".format(amopt.d['mtz'],e) )
        sys.exit(1)
    
if not amopt.d['F']:
    amopt.d['F'] = t_flag_F
    
if not amopt.d['SIGF']:
    amopt.d['SIGF'] = t_flag_SIGF
    
if not amopt.d['FREE']:
    amopt.d['FREE'] = t_flag_FREE

#
# Other checks
#
if len(pdb_code) != 4:
    print 'name is the wrong length, use 4 chars eg ABCD, changing name'
    pdb_code = 'ABCD'
else:
    pdb_code += '_'

if not os.path.exists(amopt.d['mtz']):
    print 'need mtz or no MR will be carried out'
    sys.exit()


# Print out what is being done
if amopt.d['use_buccaneer']:
    logger.info('Rebuilding in Bucaneer')
else:
    logger.info('Not rebuilding in Bucaneer')

if amopt.d['use_arpwarp']:
    logger.info('Rebuilding in ARP/wARP')
else:
    logger.info('Not rebuilding in ARP/wARP')

if amopt.d['make_frags']:
    logger.info('Making non homologusFragments')
else:
    logger.info('NOT Making Fragments')

if amopt.d['make_models']:
    logger.info('\nMaking Rosetta Models\n')
else:
    logger.info('NOT Making Rosetta Models\n')

logger.info('All needed programs are found, continuing Run')

#----------------------------
# Running is the 'official' output file 
#----------------------------
RUNNING = open(work_dir + '/ROSETTA.log', "w")
RUNNING.write(ample_util.header)
RUNNING.flush()

#----------------------------
# params used
#---------------------------
Run_params = open(work_dir + '/Params_used', "w")
param_str = amopt.prettify_parameters()
Run_params.write( param_str )
Run_params.close()

#######################################################
#
# SCRIPT PROPER STARTS HERE
#
######################################################
time_start = time.time()

#-----------------------------------
# Do The Modelling
#-----------------------------------

#-----------------------------------
# Make Rosetta fragments
#-----------------------------------
if amopt.d['make_frags']:
    rosetta_modeller.generate_fragments()

#-----------------------------------
# break here for NMR (frags needed but not modelling
# if NMR process models first
#-----------------------------------
if NMR_PROTOCOL:
    tmp = open(os.getcwd() + '/tmp.pdb', "w")
    for line in open(NMR_model_in):
        if not re.search('^HETATM', line):
            tmp.write(line)
            tmp.flush()
    tmp.close
    NMR_model_in = os.getcwd() + '/tmp.pdb'
    print 'using NMR model ' + os.getcwd() + '/tmp.pdb'

    if not amopt.d['cluster_submit']:
        os.mkdir(work_dir + '/orig_models')
        modno = split_models.split(NMR_model_in, work_dir + '/orig_models')
        os.mkdir(work_dir + '/models')
        if not amopt.d['NMR_process']:
            amopt.d['NMR_process'] = 1000 / modno
        print ' processing each model ', amopt.d['NMR_process'], ' times'
            
        homolog = work_dir + '/orig_models'
        print 'you have ', modno, ' models in your nmr'
        if modno < 2:
            if amopt.d['NMR_Truncate_only']:
                print 'cannot truncate, doing rebuilding'
            amopt.d['NMR_Truncate_only'] = False


        if not amopt.d['NMR_Truncate_only']:

            for hom in os.listdir(homolog):
                pdbs = re.split('\.', hom)

                if pdbs[-1] == 'pdb':
                    nmr.RUN_FORMAT_HOMS(homolog + '/' + hom, int(amopt.d['NMR_process']), NMR_remodel_fasta , amopt.d['rosetta_dir'], amopt.d['frags9mers'], amopt.d['frags3mers'], int(amopt.d['nproc']), hom, amopt.d['alignment_file'], work_dir + '/models')
                amopt.d['make_models'] = False
                amopt.d['models_dir'] = work_dir + '/models'
                amopt.d['models_dir'] = work_dir + '/models'
                
        if amopt.d['NMR_Truncate_only']:
            print 'Truncating NMR'
            if modno < 2:
                print 'cant trucate less than 2 models, use NMR_truncate_only False'
                sys.exit()
            if modno >= 2:
                amopt.d['make_models'] = False
                amopt.d['models_dir'] = work_dir + '/orig_models'
                amopt.d['models_dir'] = work_dir + '/orig_models'
        print 'using models from ' + amopt.d['models_dir']
    if amopt.d['cluster_submit']:
        os.mkdir(work_dir + '/orig_models')
        modno = split_models.split(NMR_model_in, work_dir + '/orig_models')
        if modno < 2:
            if amopt.d['NMR_Truncate_only']:
                print 'cannot truncate, doing rebuilding'
            amopt.d['NMR_Truncate_only'] = False


        if amopt.d['NMR_Truncate_only']:
            amopt.d['make_models'] = False
            amopt.d['models_dir'] = work_dir + '/orig_models'
            amopt.d['models_dir'] = work_dir + '/orig_models'
            print 'using models from ' + amopt.d['models_dir']


        if not amopt.d['NMR_Truncate_only']:
            amopt.d['make_models'] = False
            amopt.d['models_dir'] = work_dir + '/models'
            amopt.d['models_dir'] = work_dir + '/models'
            os.mkdir(work_dir + '/models')
            homolog = work_dir + '/orig_models'
            hom_index = 1
            if not amopt.d['NMR_process']:
                amopt.d['NMR_process'] = 1000 / modno

            homindeces = []
            for hom in os.listdir(homolog):

                os.mkdir(work_dir + '/Run_' + str(hom_index))

                pdbs = re.split('\.', hom)
                if pdbs[-1] == 'pdb':
                    ideal_homolog, ALI = nmr.CLUSTER_RUN_FORMAT_HOMS(homolog + '/' + hom, int(amopt.d['NMR_process']), NMR_remodel_fasta , amopt.d['rosetta_dir'], amopt.d['frags9mers'], amopt.d['frags3mers'], int(amopt.d['nproc']), hom, amopt.d['alignment_file'], work_dir + '/models')
                    homindeces.append(hom_index)
                    hom_index += 1

                    # Invoke the cluster run class

            for  hom_index in homindeces:
                cluster_run = clusterize.ClusterRun()
                cluster_run.QTYPE = "SGE"
                cluster_run.ALLATOM = amopt.d['allatom']
                cluster_run.setupModellingDir(work_dir + '/Run_' + str(hom_index))
                if amopt.d['use_scwrl']:
                    cluster_run.setScwrlEXE(amopt.d['scwrl_exe'])
                cluster_run.set_USE_SCWRL(amopt.d['use_scwrl'])
            # loop over the number of models and submit a job to the cluster

                for i in range(int(amopt.d['NMR_process'])):
                    cluster_run.NMRmodelOnCluster(work_dir + '/Run_' + str(hom_index), 1, i, amopt.d['rosetta_path'], amopt.d['rosetta_db'], amopt.d['fasta'], amopt.d['frags3mers'], amopt.d['frags9mers'], ideal_homolog, ALI, i, ROSETTA_CM)

            # Monitor the cluster queue to see when all jobs have finished
                cluster_run.monitorQueue()

            #  homindeces.append(hom_index)
            #  hom_index+=1
            # cluster_run.monitorQueue()
            print homindeces
            for homindex in homindeces:
                for remodelled in os.listdir(work_dir + '/Run_' + str(homindex) + '/models'):
                    remodelled_n = remodelled.rstrip('.pdb')
                    shutil.copyfile(work_dir + '/Run_' + str(homindex) + '/models/' + remodelled, work_dir + '/models/' + remodelled_n + '_' + str(homindex) + '.pdb')


    # check for same length
    l = []
    for pdb in os.listdir(amopt.d['models_dir']):
        i = split_models.check(amopt.d['models_dir'] + '/' + pdb)
        l.append(i)
        if len(l) > 1:
            mina = min(l, key=int)
            maxa = max(l, key=int)
    if mina != maxa:
        print 'min length = ', mina, ' max length = ', maxa, 'models are not equal length'
        print 'All of the models need to be the same length, All long and short models will be deleted, next time  maybe try one model at a time '

        lVals = l
        modeal_l = max(map(lambda val: (lVals.count(val), val), set(lVals)))
        modeal_l = modeal_l[1]
        print modeal_l

        for pdb in os.listdir(amopt.d['models_dir']):
            i = split_models.check(amopt.d['models_dir'] + '/' + pdb)
            if i != modeal_l:
                os.remove(amopt.d['models_dir'] + '/' + pdb)
# return from nmr with models already made

#-----------------------------------
# Make the models
#-----------------------------------
if amopt.d['make_models']:
    
    logger.info('----- making Rosetta models--------\n')
    logger.info('making ' + str(amopt.d['nmodels']) + ' models...')
    RUNNING.write('\n----- making models--------\n')
    RUNNING.flush()
    
    # If we are running with cluster support submit all modelling jobs to the cluster queue
    if amopt.d['cluster_submit']:
        
        # Generate the list of random seeds
        rosetta_modeller.generate_seeds()

        # Invoke the cluster run class
        cluster_run = clusterize.ClusterRun()
        cluster_run.QTYPE = amopt.d['submit_qtype']
        cluster_run.setModeller(rosetta_modeller)
        cluster_run.setupModellingDir(work_dir)
        
        # loop over the number of models and submit a job to the cluster
        for i in range(amopt.d['nmodels']):
            nproc=1 # each modelling job runs on a single processor
            jobNumber=i+1
            cluster_run.modelOnCluster( nproc, jobNumber )

        # Monitor the cluster queue to see when all jobs have finished
        cluster_run.monitorQueue()

    else:
        # Run locally
        amopt.d['models_dir'] = rosetta_modeller.doModelling()
        
    ##End IF amopt.d['cluster_submit']
    
RUNNING.write('\nModelling complete - models stored in:\n   ' + amopt.d['models_dir'] + '\n\n')
logger.info('Modelling complete - models stored in:\n   ' + amopt.d['models_dir'] + '\n\n')

#--------------------------------------------
# check if models are present regardless
#--------------------------------------------
if amopt.d['import_cluster']:
    if os.path.exists(amopt.d['import_cluster']): 
        cluster_path = amopt.d['import_cluster']
        if not os.path.exists(work_dir + '/fine_cluster_1'):
            os.mkdir(work_dir + '/fine_cluster_1')
        os.chdir(work_dir + '/fine_cluster_1')
        list_of_ensembles = truncateedit_MAX.truncate(amopt.d['theseus_exe'], cluster_path , work_dir + '/fine_cluster_1', amopt.d['maxcluster_exe'], percent, False)

        os.system('mkdir ' + work_dir + '/ensembles_1')
        for each_ens in list_of_ensembles:
            SCWRL_edit.edit_sidechains(each_ens, work_dir + '/ensembles_1/')
        sys.exit()


#---------------------------------------
# Do the clustering
#---------------------------------------
ensembles = [] # List of ensembles - 1 per cluster
if not amopt.d['import_ensembles']:
    #
    logger.info('----- Clustering models --------')
    
    # Spicker Alternative for clustering then MAX
    run_spicker.RUN_SPICKER(amopt.d['models_dir'], work_dir + '/spicker_run', amopt.d['spicker_exe'], amopt.d['num_clusters'] , work_dir)
    
    # Generate list of clusters to sample
    # Done like this so it's easy to run specific clusters
    cluster_nums = [ i for i in range(1,amopt.d['num_clusters']+1)]
    
    logger.info('Clustering Done. Using the following clusters: {0}'.format(cluster_nums) )
    
    for cluster in cluster_nums:
        models_path = work_dir + '/S_clusters/cluster_' + str(cluster)
        logger.info('----- Truncating models for cluster ' + str(cluster) + ' --------\n')
        if not os.path.exists(work_dir + '/fine_cluster_' + str(cluster)):
            os.mkdir(work_dir + '/fine_cluster_' + str(cluster))
    
        os.chdir(work_dir + '/fine_cluster_' + str(cluster))
        #
        # Truncate models and return the path to the ensembled models - each ensemble is a single pdb file
        # containing multiple models
        if amopt.d['ensembler']:
            list_of_ensembles = truncateedit_MAX.truncate_Phenix(amopt.d['phenix_exe'], models_path, work_dir + '/fine_cluster_' + str(cluster), amopt.d['maxcluster_exe'], percent, FIXED_INTERVALS)
        else:
            list_of_ensembles = truncateedit_MAX.truncate(amopt.d['theseus_exe'], models_path, work_dir + '/fine_cluster_' + str(cluster), amopt.d['maxcluster_exe'], percent, FIXED_INTERVALS)
    
        if amopt.d['top_model_only']:
            list_of_ensembles = truncateedit_MAX.One_model_only(list_of_ensembles, work_dir)
    
        os.mkdir(work_dir + '/ensembles_' + str(cluster))
        for each_ens in list_of_ensembles:
            SCWRL_edit.edit_sidechains(each_ens, work_dir + '/ensembles_' + str(cluster) + '/')
    
        final_ensembles = []
        for infile in glob.glob(os.path.join(work_dir + '/ensembles_' + str(cluster), '*.pdb')):
            final_ensembles.append(infile)
            
        # Add to the total list
        ensembles.append(final_ensembles)
        
        logger.info('Truncating Done for cluster ' + str(cluster) + "\n\n")
        logger.info('----- Running MRBUMP (cluster ' + str(cluster) + ')--------\n\n')
        logger.info('Created ' + str(len(final_ensembles)) + ' ensembles,   running ' + str(len(final_ensembles)) + ' jobs for cluster ' + str(cluster) + '\n')
    
        # Start MR BUMP analysis of ensemble
        bump_dir = os.path.join(work_dir, 'MRBUMP')
        if not os.path.exists(bump_dir):
            os.mkdir(bump_dir)
        os.chdir(bump_dir)
        amopt.d['mrbump_dir'] = bump_dir
        
        if amopt.d['cluster_submit']:
            mrbump_ensemble.mrbump_ensemble_cluster( final_ensembles, amopt.d, clusterID=cluster)
        else:
            mrbump_ensemble.mrbump_ensemble_local( final_ensembles, amopt.d, clusterID=cluster)
        # End loop over clusters

#---------------------------------------
# Importing pre-made ensembles
#---------------------------------------
if amopt.d['import_ensembles']:
    
    # Set list of ensembles to the one we are importing
    logger.info("Importing existing ensembles from: {0}".format( amopt.d['ensembles'] ) )
    final_ensembles = []
    for infile in glob.glob(os.path.join(amopt.d['ensembles'], '*.pdb')):
        final_ensembles.append(infile)

    if amopt.d['top_model_only']:
        logger.info("Only using the top model" )
        final_ensembles = truncateedit_MAX.One_model_only(final_ensembles , amopt.d['work_dir'])
        
    # Set ensembles
    ensembles = [ final_ensembles ]

#---------------------------------------
# MR BUMP analysis of the ensembles
#---------------------------------------
bump_dir = os.path.join(work_dir, 'MRBUMP')
logger.info("Running MRBUMP in directory: {0}".format( bump_dir ) )
if not os.path.exists(bump_dir):
    os.mkdir(bump_dir)
os.chdir(bump_dir)
amopt.d['mrbump_dir'] = bump_dir

# Loop over clusters
for i, ensemble in enumerate(ensembles):
    logger.info("Running MRBUMP for cluster: {0}".format( i ) )
    if amopt.d['cluster_submit']:
        mrbump_ensemble.mrbump_ensemble_cluster( ensemble, amopt.d)
    else:
        mrbump_ensemble.mrbump_ensemble_local( ensemble, amopt.d)

# Timing data
time_stop = time.time()
elapsed_time = time_stop - time_start
run_in_min = elapsed_time / 60
run_in_hours = run_in_min / 60
msg = '\nMR and shelx DONE \n ALL DONE  (in ' + str(run_in_hours) + ' hours) \n----------------------------------------\n'
logging.info(msg)
RUNNING.write(msg)
RUNNING.flush()

# os.system('tar -cvf '+pdb_code+'_' + work_dir+'.tar '+work_dir)
# os.system('gzip ' +pdb_code+'_'+ work_dir+'.tar')
# os.system('mv '+ pdb_code+'_'+  work_dir+'.tar.gz /data2/jac45 ')
# os.system('rm '+work_dir)
sys.exit(0)
#------------------------------------
# END
#----------------------------------
