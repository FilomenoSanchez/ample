#!/usr/bin/env python

"""
Version 0.1
no speed buffs, no extra functionality, no optimisation
Basic Pipeline
for multi core desktops, not clusters

Issues:
1) if this script is killed, the rosetta will continue
2) Theseus can hang, this is killed after a timeout
3) is clustered by all atom RMSD to make fine clusters (option to cluster by CA only i included)
4) ASU content is the number of search models placed by MrBUMP. -- need to set this manually

jmht

make_fragments
nmr_modelling_step
make_models

clusterimport step - does this do anything?

if not ENSEMBLE_IMPORT:
    cluster_models
    foreach cluster:
        generate_ensembles
        edit sidechains
        
        if not MISSING_DOMAINS:
            run mrbump
        else
            run mrbump

# seems to exit at all these places
The reruns again with imported clusters below

================================

so...
Only data passed between different stages are the pdb files, so each stage could just start from a directory of files

"""

import os
import sys

# Test for environment variables

if not "CCP4" in sorted(os.environ.keys()):
    raise RuntimeError('CCP4 not found')

# Add the ample python folder to the PYTHONPATH
sys.path.append(os.path.join(os.environ["CCP4"], "share", "ample", "python"))

# python imports
import argparse
import clusterize
import glob
import logging
import random
import re
import shutil
import string
import subprocess
import time

# Our imports
import add_sidechains_SCWRL
import ample_options
import ample_util
import fasta_parser
import printTable
import rosetta_model
import run_mr_bump_shelx_parallel
import run_spicker
import SCWRL_edit, Final_display_results
import split_models, nmr
import truncateedit_MAX


#-------------------------------------------
# get command line options
#-------------------------------------------

parser = argparse.ArgumentParser(description='Structure solution by abinitio modelling', prefix_chars="-")

#
# Rosetta arguments
#
parser.add_argument('-rosetta_db', metavar='ROSETTA_database', type=str, nargs=1,
                   help='path for Rosetta database')

parser.add_argument('-ROSETTA_cluster', metavar='path to Rosettas cluster', type=str, nargs=1,
                   help='location of rosetta cluster')

parser.add_argument('-ROSETTA', metavar='ROSETTA_path', type=str, nargs=1,
                   help='path for Rosetta AbinitioRelax')

parser.add_argument('-rosetta_dir', metavar='Rosetta_dir', type=str, nargs=1,
                   help='the Rosetta install directory')

# Rosetta fragments
parser.add_argument('-rosetta_fragments_exe', metavar='path to make_fragments.pl', type=str, nargs=1,
                   help='location of make_fragments.pl')

parser.add_argument('-make_frags', metavar='bool to make fragments', type=str, nargs=1,
                   help='Bool, True to make non homologous framents, False to import fragments')

parser.add_argument('-frags3mers', metavar='frags3mers', type=str, nargs=1,
                   help='path of imported 3mers')

parser.add_argument('-frags9mers', metavar='frags9mers', type=str, nargs=1,
                   help='path of imported 9mers')

parser.add_argument('-fasta', metavar='fasta_file', type=str, nargs=1,
                   help='protein fasta file. (required)')

parser.add_argument('-name', metavar='priotein name', type=str, nargs=1,
                   help='name of protien in the format ABCD ')

parser.add_argument('-nproc', metavar='NoProcessors', type=int, nargs=1,
                   help='number of processers (default 1)')


parser.add_argument('-run_dir', metavar='run_directory', type=str, nargs=1,
                   help='directory to put files (default current dir)')


parser.add_argument('-scwrl_exe', metavar='path to scwrl', type=str, nargs=1,
                   help='pathway to SCWRL exe')


parser.add_argument('-LGA', metavar='path_to_LGA dir', type=str, nargs=1,
                   help='pathway to LGA folder (not the exe) will use the \'lga\' executable')


parser.add_argument('-MAX', metavar='Maxcluster exe', type=str, nargs=1,
                   help='Maxcluster exe')


parser.add_argument('-THESEUS', metavar='Theseus exe (required)', type=str, nargs=1,
                   help='Theseus exe')

parser.add_argument('-PHENIX', metavar='PHENIX.ensembler exe', type=str, nargs=1,
                   help='PHENIX exe')

parser.add_argument('-mtz', metavar='MTZ in', type=str, nargs=1,
                   help='MTZ in')


parser.add_argument('-models_dir', metavar='folder of decoys', type=str, nargs=1,
                   help='folder of decoys')

parser.add_argument('-make_models', metavar='Do the modelling', type=str, nargs=1,
                   help='run rosetta modeling, set to False to import pre-made models (required if making models locally default True)')


# # FLAGS
parser.add_argument('-F', metavar='flag for F', type=str, nargs=1,
                   help='Flag for F')

parser.add_argument('-SIGF', metavar='flag for SIGF', type=str, nargs=1,
                   help='Flag for SIGF')

parser.add_argument('-FREE', metavar='flag for FREE', type=str, nargs=1,
                   help='Flag for FREE')

parser.add_argument('-CLUSTER', metavar='using a cluster', type=str, nargs=1,
                   help='will submit jobs to a cluster')

parser.add_argument('-allatom', metavar='using a cluster', type=str, nargs=1,
                   help='will submit jobs to a cluster')

parser.add_argument('-SPICKER', metavar='boolean', type=str, nargs=1,
                   help='path and will use spicker as alternative')

parser.add_argument('-SPICKERPATH', metavar='spicker exe', type=str, nargs=1,
                   help='path and will use spicker as alternative')


parser.add_argument('-domain_all_chains_pdb', metavar='domain_all_chains_pdb', type=str, nargs=1,
                   help='fixed input to mr bump')

parser.add_argument('-domain_all_chain_fasta', metavar='domain_all_chain_fasta', type=str, nargs=1,
                   help='fasta for all ASU')

parser.add_argument('-domain_termini_distance', metavar='termini', type=str, nargs=1,
                   help='distance between termini for insert domains')


parser.add_argument('-nmodels', metavar='no models', type=int, nargs=1,
                   help='number of models to make (1000)')


parser.add_argument('-CC', metavar='rad of gyration', type=str, nargs=1,
                   help='radius of gyration reweight ')

parser.add_argument('-ensembles', metavar='enembles', type=str, nargs=1,
                   help='path to ensembles')

parser.add_argument('-noClusters', metavar='no clus to sample', type=str, nargs=1,
                   help='number of clusters to sample')


parser.add_argument('-percent', metavar='no clus to sample', type=str, nargs=1,
                   help='percent interval for truncation')

parser.add_argument('-ASU', metavar='no in ASU', type=int, nargs=1,
                   help='no in ASU')


# jmht - not used?
#parser.add_argument('-FreeLunch', metavar='free lunch', type=int, nargs=1,
#                   help='true to use free lunch, false to not use it')

parser.add_argument('-usehoms', metavar='nohoms rosetta flag', type=str, nargs=1,
                   help='True =use nhomologs, False= dont use them ')

parser.add_argument('-improve_template', metavar='improve template', type=str, nargs=1,
                   help='give a template to imrove - NMR, homolog ')

parser.add_argument('-DEBUG', metavar='debug option', type=str, nargs=1,
                   help='debug option ')

parser.add_argument('-ensembler', metavar='ensembler', type=str, nargs=1,
                   help='ensembling')

parser.add_argument('-early_terminate', metavar='early_terminate', type=str, nargs=1,
                     help='terminate early if a success')

parser.add_argument('-shelx_cycles', metavar='shelx_cycles', type=str, nargs=1,
                     help='number of shelx sycles')


parser.add_argument('-molreponly', metavar='molreponly', type=str, nargs=1,
                   help='molreponly')

parser.add_argument('-phaseronly', metavar='phaseronly', type=str, nargs=1,
                   help='phaseronly')

parser.add_argument('-clusterimport', metavar='clusterimport', type=str, nargs=1,
                   help='clusterimport')

parser.add_argument('-old_shelx', metavar='old_shelx', type=str, nargs=1,
                   help='old_shelx')

parser.add_argument('-top_model_only', metavar='top_model_only', type=str, nargs=1,
                   help='top_model_only')

parser.add_argument('-use_scwrl', metavar='USE_SCWRL', type=str, nargs=1,
                   help='if using scwrl true if not false')

parser.add_argument('-alignment_file', metavar='alignment_file', type=str, nargs=1,
                   help='alignment between homolog and target fastas')

parser.add_argument('-NMR_model_in', metavar='NMR_model_in', type=str, nargs=1,
                   help='use nmr input')

parser.add_argument('-NMR_process', metavar='NMR_process', type=str, nargs=1,

                   help='number of times to process the models')

parser.add_argument('-NMR_remodel_fasta', metavar='-NMR_remodel_fasta', type=str, nargs=1,
                   help='fasta_for_remodeling')

parser.add_argument('-NMR_Truncate_only', metavar='-NMR_Truncate_only', type=str, nargs=1,
                   help='do no remodelling only truncate the NMR')

parser.add_argument('-use_buccaneer', metavar='use_buccaneer', type=str, nargs=1,
                   help='True to use Buccaneer')

parser.add_argument('-buccaneer_cycles', metavar='buccaneer_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-arpwarp_cycles', metavar='arpwarp_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-use_arpwarp', metavar='-use_arpwarp', type=str, nargs=1,
                   help='True to use arpwarp ')

parser.add_argument('-use_shelxe', metavar='-use_shelxe', type=str, nargs=1,
                   help='True to use shelx ')

parser.add_argument('-mr_keys', metavar='-mr_keys', type=str, nargs=1,
                   help='mrbump keywords ')

#
# jmht - set the default values here
#
# name -> pdb_code
# SCRWL -> SCWRL_path
# MAX -> MAX_path
parser.set_defaults( 
                    alignment_file = '',
                    allatom = "true",
                    arpwarp_cycles = 10,
                    ASU = 0,
                    buccaneer_cycles = 5,
                    CC = '',
                    CLUSTER = "false",
                    clusterimport = "false",
                    DEBUG = "false",
                    domain_all_chains_pdb = None,
                    domain_all_chains_fasta = None,
                    domain_termini_distance = 0,
                    ensembles = None,
                    ensembler = "false",
                    fasta = 'no_fasta_given',
                    frags3mers=None,
                    frags9mers=None,
                    improve_template = None,
                    MAX = '',
                    make_frags = "true",
                    make_models = "true",
                    models_dir = '',
                    molreponly = "false",
                    mtz = None,
                    name = 'ABCD',
                    noClusters = 1,
                    nmodels = 1000,
                    NMR_model_in = '',
                    NMR_process = 'none',
                    NMR_remodel_fasta = '',
                    NMR_Truncate_only = "false",
                    nproc = 1,
                    NumShelxCyc = 15,
                    old_shelx = "false",
                    percent = 5,
                    phaseronly = "false",
                    PHENIX = '',
                    rosetta_cluster = None,
                    rosetta_db  = None,
                    rosetta_fragments_exe = None,
                    rosetta_path =  None,
                    run_dir = os.getcwd(),
                    scwrl_exe = '',
                    SPICKER = "true",
                    SPICKERPATH = '',
                    usehoms = "false",
                    use_arpwarp = "true",
                    use_buccaneer = "true",
                    use_scwrl = 'false',
                    use_shelxe = "false",
                    top_model_only = "false",
                    THESEUS = '',
                    early_terminate = "true"
                    )


# convert args to dictionary
args = parser.parse_args()

# Set up logging
logger = ample_util.setup_logging()


# get MRBUMP keywords direct
MRkeys = []
# print sys.argv
keycount = 0
while keycount < len(sys.argv):
            # print sys.argv[keycount] ,  keycount
    if sys.argv[keycount] == "-mr_keys":
        MRkeys.append(sys.argv[keycount + 1])
    keycount += 1


# jmht will remove
var_args = vars(args)

# Populate the options from the parser
amopt = ample_options.AmpleOptions( )
amopt.populate( args )

# Now set MRKeys
amopt.d['mr_keys'] = MRkeys

#
# Set variables - will use dict directly in future
#
alignment_file = amopt.d['alignment_file']
ALLATOM = amopt.d['allatom']
# jmht - runon_cluster
CLUSTER = amopt.d['CLUSTER']
clusterimport = amopt.d['clusterimport']
DEBUG = amopt.d['DEBUG']
Ensembler = amopt.d['ensembler']
FASTA = amopt.d['fasta']
FIXED_INPUT = True
MAX_path = amopt.d['MAX']
molreponly = amopt.d['molreponly'] 
noClusters = amopt.d['noClusters']
nmodels = amopt.d['nmodels']
NMR_process = amopt.d['NMR_process']
NMR_Truncate_only = amopt.d['NMR_Truncate_only']
pdb_code = amopt.d['name']
#jmht - fix
amopt.d['pdb_code'] = amopt.d['name']
phaseronly = amopt.d['phaseronly']
PHENIX_path = amopt.d['PHENIX']
run_dir = amopt.d['run_dir']
ROSETTA_cluster = amopt.d['ROSETTA_cluster']
SCWRL_path = amopt.d['scwrl_exe']
SPICKER = amopt.d['SPICKER']
SPICKEREXE_path = amopt.d['SPICKERPATH']
THESEUS_path = amopt.d['THESEUS']
top_model_only = amopt.d['top_model_only']
USE_SCWRL = amopt.d['use_scwrl']
USE_SPICKER = True


# Make a work directory and go there
work_dir = ample_util.make_workdir( run_dir )
amopt.d['work_dir'] = work_dir
os.chdir(work_dir)

#
# All the variables set below here need some sort of processing
# Should probably think of sensible ways to set defaults or
# run the checks in a different place
#
if amopt.d['ASU'] > 0:
    amopt.d['ASU'] = 'NMASU ' + str( amopt.d['ASU']  )
else:
    amopt.d['ASU'] = str(amopt.d['ASU'])

FIXED_INTERVALS = True
if amopt.d['percent'] != 5:
    FIXED_INTERVALS = False
percent = amopt.d['percent']

MISSING_DOMAINS = False
if amopt.d['domain_all_chains_pdb'] or amopt.d['domain_all_chains_fasta']:
    MISSING_DOMAINS = True

NMR_PROTOCOL = False
if os.path.isfile( amopt.d['NMR_model_in'] ):
    NMR_model_in = amopt.d['NMR_model_in']
    NMR_PROTOCOL = True

if os.path.isfile( amopt.d['NMR_remodel_fasta'] ):
    NMR_remodel_fasta = amopt.d['NMR_remodel_fasta']
else:
    NMR_remodel_fasta = amopt.d['fasta']


amopt.d['mrbump_programs'] = ' molrep phaser '
if not molreponly and not phaseronly:
    amopt.d['mrbump_programs'] = ' molrep phaser '
if molreponly:
    amopt.d['mrbump_programs'] = ' molrep  '
if phaseronly:
    amopt.d['mrbump_programs'] = ' phaser  '
if molreponly and phaseronly:
        logger.critical('you say you want molrep only AND phaser only, choose one or both')
        sys.exit()    
    

# #flags
# Check if we need to get any of the flags from the MTZ file
if not amopt.d['F'] or not amopt.d['SIGF'] or not amopt.d['FREE']:
    
    if not os.path.isfile( amopt.d['mtz'] ):
        logger.critical("Cannot find MTZ file: {}".format( amopt.d['mtz'] ) )
        sys.exit(1)
    try:
        t_flag_F, t_flag_SIGF, t_flag_FREE = ample_util.get_mtz_flags( amopt.d['mtz'] )
    except KeyError,e:
        logger.critical("Error generating flags from MTZ file: {}\n{}\nYou may need to run the CCP4 uniqueify on the MTZ file".format(amopt.d['mtz'],e) )
        sys.exit(1)
    
if not amopt.d['F']:
    amopt.d['F'] = t_flag_F
    
if not amopt.d['SIGF']:
    amopt.d['SIGF'] = t_flag_SIGF
    
if not amopt.d['FREE']:
    amopt.d['FREE'] = t_flag_FREE


ENSEMBLE_import = False
if os.path.isdir( amopt.d['ensembles'] ):
    ENSEMBLE_import = True
    logger.info("Found directory with ensemble files: {}\nSetting make_models to False".format( amopt.d['ensembles'] ) )
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False
    
amopt.d['importing_models'] = False
if os.path.isdir( amopt.d['models_dir'] ):
    amopt.d['importing_models'] = True
    # jmht FIX need to not make them
    amopt.d['make_models'] = False

# check if got all the programs
if not amopt.d['make_models']:
    amopt.d['importing_models'] = True

if amopt.d['make_models'] and amopt.d['importing_models']:
    print amopt.d['models_dir']
    logger.critical('you have chosen to both import models and to make them, choose only one\nSet Make Models to \"True\" to make them, \"False\" to import them')
    sys.exit()

if not  NMR_PROTOCOL :
    if amopt.d['importing_models'] and not os.path.exists(amopt.d['models_dir']):
        logger.warn('you have chosen to import models, but path does not exist, looking for ensembles : ' + amopt.d['models_dir'])
        if not os.path.exists(amopt.d['ensembles']):
            logger.warn('you have chosen to import ensembles, but path does not exist : ' + amopt.d['ensembles'])
            if clusterimport:
                logger.info('you are importing a cluster of models from :' + clusterimport)
                if not os.path.exists(clusterimport):
                    logger.critical('you have chosen to import a cluster, the clusterpath does not exist')
                    sys.exit()

# Create the rosetta modeller - this runs all the checks required
if amopt.d['make_models'] or amopt.d['make_frags']:  # only need Rosetta if making models
    rosetta_modeller = rosetta_model.RosettaModel()
    rosetta_modeller.set_from_amopt( amopt )

if NMR_PROTOCOL:
    ROSETTA_CM = amopt.d['rosetta_dir'] + '/rosetta_source/bin/mr_protocols.default.linuxgccrelease'

    if not os.path.exists(amopt.d['rosetta_db']) :
        print ' cant find ' + amopt.d['rosetta_db'] + ' check path names'
        sys.exit()

    if not os.path.exists(ROSETTA_CM) :
        print ' cant find ' + ROSETTA_CM + ' check path names'
        sys.exit()


if not os.path.exists(work_dir):
    print 'You need to give a run directory'
    sys.exit()


if not Ensembler:
    THESEUS = ample_util.check_for_exe('theseus', THESEUS_path)
        # print 'finally got', THESEUS
if Ensembler:
    print 'You are using Phenix ensmbler'
    PHENIX = ample_util.check_for_exe('phenix', PHENIX_path)
    
if USE_SCWRL:
    SCWRL = ample_util.check_for_exe('Scwrl4', SCWRL_path)
    # print 'finally got', SCWRL
if not USE_SCWRL:
    SCWRL = ''
    
SPICKEREXE = ample_util.check_for_exe('spicker', SPICKEREXE_path)

# print 'finally got', SPICKEREXE
MAX = ample_util.check_for_exe('maxcluster', MAX_path)
# print 'finally got', MAX
if amopt.d['use_shelxe']:
    shelxe = ''
    shelxe = ample_util.check_for_exe('shelxe', shelxe)


if len(pdb_code) > 4 or len(pdb_code) < 4:
    print 'name is the wrong length, use 4 chars eg ABCD, changing name'
    pdb_code = 'ABCD'


if len(pdb_code) == 4:
    pdb_code += '_'

logger.debug('Parsing FASTA file')
if not os.path.exists(FASTA):
    print 'You need to give the path for the fasta'
    sys.exit()
outfasta = os.path.join(work_dir, pdb_code + '_.fasta')
fasta_parser.parse_fasta(FASTA, outfasta)
FASTA = outfasta



if not os.path.exists(amopt.d['mtz']):
    print 'need mtz or no MR will be carried out'
    sys.exit()


#
# End of setting variables
#

# Print out what is being done - sorta..
if amopt.d['use_buccaneer']:
    logger.info('Rebuilding in Bucaneer')
else:
    logger.info('Not rebuilding in Bucaneer')

if amopt.d['use_arpwarp']:
    logger.info('Rebuilding in ARP/wARP')
else:
    logger.info('Not rebuilding in ARP/wARP')


if amopt.d['make_frags']:
    logger.info('Making non homologusFragments')
else:
    logger.info('NOT Making Fragments')

if amopt.d['make_models']:
    logger.info('\nMaking Rosetta Models\n')
else:
    logger.info('NOT Making Rosetta Models\n')
    

# This should go somewhere else
LOG = open(work_dir + '/Ample.log', "a")
LOG.write('The authors of specific programs should be referenced where applicable::\n')
LOG.write(ample_util.references)
LOG.close()

RUNNING = open(work_dir + '/ROSETTA.log', "w")

RUNNING.write("""#########################################################################
#########################################################################
#########################################################################
# CCP4: AMPLE -Ab Initio Modelling Molecular Replacement (Beta version) #
#########################################################################
The authors of specific programs should be referenced where applicable:""")
RUNNING.write(ample_util.references)
RUNNING.flush()

logger.info('All needed programs are found, continuing Run')


#----------------------------
# params used
#---------------------------
# make a copy of params
#

Run_params = open(work_dir + '/Params_used', "w")
Run_params.write('input params\n')
if DEBUG:
    print var_args
    for print_user in var_args:
        if var_args[print_user] is not None:
            try:
                print print_user + ' : ' + str(var_args[print_user][0])
                Run_params.write(print_user + ' : ' + str(var_args[print_user][0]) + '\n')
            except:
                pass


Run_params.write('\nParams Used in this Run\n')
Run_params.write('\n---input---\nFasta ' + FASTA + '\nrun_dir ' + work_dir + '\nMTZ ' + amopt.d['mtz'] + '\nname ' + pdb_code + '\n')
Run_params.write('\n---fragments---\nMakeFrags ' + str(amopt.d['make_frags']) + '\n3mers ' + amopt.d['frags3mers'] + '\n9mers ' + amopt.d['frags9mers'] + '\n')
Run_params.write('\n---modelling---\nmake_models ' + str(amopt.d['make_models']) + '\nROSETTA_PATH ' +  str(amopt.d['rosetta_path']) + '\n')
Run_params.write('ROSETTA_cluster ' + str(ROSETTA_cluster) + '\nROSETTA_DB ' + str(amopt.d['rosetta_db']) + '\nMake_fragments_exe ' + str(amopt.d['rosetta_fragments_exe']) + '\n')
if USE_SCWRL == True:
    Run_params.write('\n---3rd party---\nSCWRL ' + SCWRL + '\n')
Run_params.write('\n---Missing Domain---\nall chains fasta ' + str(amopt.d['domain_all_chains_fasta']) + '\nall chain pdb ' + str(amopt.d['domain_all_chains_pdb']) + '\nMISSING DOMAINS=' + str(MISSING_DOMAINS) + '\n')
# This only used for printing
INSERT_DOMAIN = False
if amopt.d['domain_termini_distance'] > 0:
    INSERT_DOMAIN = True
Run_params.write('Is an Insert Domain ' + str(INSERT_DOMAIN) + ' termini distance ' + str(amopt.d['domain_termini_distance']) + '\n')

Run_params.close()

#-----------------------------------
# Do The Modelling
#-----------------------------------
time_start = time.time()

#######################################################
#
# SCRIPT PROPER STARTS HERE
#
######################################################

# Make Rosetta fragments
if amopt.d['make_frags']:
    rosetta_modeller.generate_fragments()
           

# ## break here for NMR (frags needed but not modelling
# if NMR process models first
if NMR_PROTOCOL:
    tmp = open(os.getcwd() + '/tmp.pdb', "w")
    for line in open(NMR_model_in):
        if not re.search('^HETATM', line):
            tmp.write(line)
            tmp.flush()
    tmp.close
    NMR_model_in = os.getcwd() + '/tmp.pdb'
    print 'using NMR model ' + os.getcwd() + '/tmp.pdb'

    if not CLUSTER:
        os.mkdir(work_dir + '/orig_models')
        modno = split_models.split(NMR_model_in, work_dir + '/orig_models')
        os.mkdir(work_dir + '/models')
        if NMR_process == 'none':
            NMR_process = 1000 / modno

            print ' processing each model ', NMR_process, ' times'
        else:
            print ' processing each model ', NMR_process, ' times'
        homolog = work_dir + '/orig_models'
        print 'you have ', modno, ' models in your nmr'
        if modno < 2:
            if NMR_Truncate_only:
                print 'cannot truncate, doing rebuilding'
            NMR_Truncate_only = False


        if not NMR_Truncate_only:

            for hom in os.listdir(homolog):
                pdbs = re.split('\.', hom)

                if pdbs[-1] == 'pdb':
                    nmr. RUN_FORMAT_HOMS(homolog + '/' + hom, int(NMR_process), NMR_remodel_fasta , amopt.d['rosetta_dir'], amopt.d['frags9mers'], amopt.d['frags3mers'], int(amopt.d['nproc']), hom, alignment_file, work_dir + '/models')
                amopt.d['make_models'] = False
                PATH_TO_MODELS = work_dir + '/models'
                amopt.d['models_dir'] = work_dir + '/models'
                
        if NMR_Truncate_only:
            print 'Truncating NMR'
            if modno < 2:
                print 'cant trucate less than 2 models, use NMR_truncate_only False'
                sys.exit()
            if modno >= 2:
                amopt.d['make_models'] = False
                PATH_TO_MODELS = work_dir + '/orig_models'
                amopt.d['models_dir'] = work_dir + '/orig_models'
        print 'using models from ' + PATH_TO_MODELS
    if CLUSTER:
        os.mkdir(work_dir + '/orig_models')
        modno = split_models.split(NMR_model_in, work_dir + '/orig_models')
        if modno < 2:
            if NMR_Truncate_only == True :
                print 'cannot truncate, doing rebuilding'
            NMR_Truncate_only = False


        if NMR_Truncate_only == True:
            amopt.d['make_models'] = False
            PATH_TO_MODELS = work_dir + '/orig_models'
            amopt.d['models_dir'] = work_dir + '/orig_models'
            print 'using models from ' + PATH_TO_MODELS


        if NMR_Truncate_only == False:
            amopt.d['make_models'] = False
            PATH_TO_MODELS = work_dir + '/models'
            amopt.d['models_dir'] = work_dir + '/models'
            os.mkdir(work_dir + '/models')
            homolog = work_dir + '/orig_models'
            hom_index = 1
            if NMR_process == 'none':
                NMR_process = 1000 / modno

            homindeces = []
            for hom in os.listdir(homolog):

                os.mkdir(work_dir + '/Run_' + str(hom_index))

                pdbs = re.split('\.', hom)
                if pdbs[-1] == 'pdb':
                    ideal_homolog, ALI = nmr.CLUSTER_RUN_FORMAT_HOMS(homolog + '/' + hom, int(NMR_process), NMR_remodel_fasta , amopt.d['rosetta_dir'], amopt.d['frags9mers'], amopt.d['frags3mers'], int(amopt.d['nproc']), hom, alignment_file, work_dir + '/models')
                    homindeces.append(hom_index)
                    hom_index += 1

                    # Invoke the cluster run class

            for  hom_index in homindeces:
                cluster_run = clusterize.ClusterRun()
                cluster_run.QTYPE = "SGE"
                cluster_run.ALLATOM = ALLATOM
                cluster_run.setupModellingDir(work_dir + '/Run_' + str(hom_index))
                if USE_SCWRL:
                    cluster_run.setScwrlEXE(SCWRL)
                cluster_run.set_USE_SCWRL(USE_SCWRL)
            # loop over the number of models and submit a job to the cluster

                for i in range(int(NMR_process)):
                    cluster_run.NMRmodelOnCluster(work_dir + '/Run_' + str(hom_index), 1, i, amopt.d['rosetta_path'], amopt.d['rosetta_db'], FASTA, amopt.d['frags3mers'], amopt.d['frags9mers'], ideal_homolog, ALI, i, ROSETTA_CM)

            # Monitor the cluster queue to see when all jobs have finished
                cluster_run.monitorQueue()

                           #  homindeces.append(hom_index)
                           #  hom_index+=1
            # cluster_run.monitorQueue()
            print homindeces
            for homindex in homindeces:
                for remodelled in os.listdir(work_dir + '/Run_' + str(homindex) + '/models'):
                    remodelled_n = remodelled.rstrip('.pdb')
                    shutil.copyfile(work_dir + '/Run_' + str(homindex) + '/models/' + remodelled, work_dir + '/models/' + remodelled_n + '_' + str(homindex) + '.pdb')


    # check for same length
    l = []
    for pdb in os.listdir(amopt.d['models_dir']):
        i = split_models.check(amopt.d['models_dir'] + '/' + pdb)
        l.append(i)
        if len(l) > 1:
            mina = min(l, key=int)
            maxa = max(l, key=int)
    if mina != maxa:
        print 'min length = ', mina, ' max length = ', maxa, 'models are not equal length'
        print 'All of the models need to be the same length, All long and short models will be deleted, next time  maybe try one model at a time '

        lVals = l
        modeal_l = max(map(lambda val: (lVals.count(val), val), set(lVals)))
        modeal_l = modeal_l[1]
        print modeal_l

        for pdb in os.listdir(amopt.d['models_dir']):
            i = split_models.check(amopt.d['models_dir'] + '/' + pdb)
            if i != modeal_l:
                os.remove(amopt.d['models_dir'] + '/' + pdb)


# return from nmr with models already made


print  '----- making Rosetta models--------\n'
print 'making ' + str(nmodels) + ' models...'
RUNNING.write('----- making models--------\n')
RUNNING.flush()

if amopt.d['make_models']:
    PATH_TO_MODELS = work_dir + '/models'
    
    # If we are running with cluster support submit all modelling jobs to the cluster queue
    if CLUSTER:
        # Generate the list of random seeds
        rosetta_modeller.generate_seeds()

        # Invoke the cluster run class
        cluster_run = clusterize.ClusterRun()
        cluster_run.QTYPE = "SGE"
        cluster_run.setModeller(rosetta_modeller)
        cluster_run.setupModellingDir(work_dir)
        
        # loop over the number of models and submit a job to the cluster
        for i in range(nmodels):
            #cluster_run.modelOnCluster(work_dir, 1, i + 1, ROSETTA_PATH, ROSETTA_DB, FASTA, amopt.d['frags3mers'], amopt.d['frags9mers'], seed_list[i], insert_Rosetta_command + CCline)
            nproc=1
            jobNumber=i+1
            cluster_run.modelOnCluster( nproc, jobNumber )

        # Monitor the cluster queue to see when all jobs have finished
        cluster_run.monitorQueue()

    else:
        # Run locally
        PATH_TO_MODELS = rosetta_modeller.doModelling()
        
    ##End IF CLUSTER
else:
    # Get existing models
    PATH_TO_MODELS = amopt.d['models_dir']
    
RUNNING.write('\nModelling complete - models stored in:\n   ' + PATH_TO_MODELS + '\n\n')
logger.info('Modelling complete - models stored in:\n   ' + PATH_TO_MODELS + '\n\n')


#--------------------------------------------
# check if models are present regardless
#--------------------------------------------
if clusterimport:
    if os.path.exists(clusterimport): 
        cluster_path = clusterimport
        if not os.path.exists(work_dir + '/fine_cluster_1'):
            os.mkdir(work_dir + '/fine_cluster_1')
        os.chdir(work_dir + '/fine_cluster_1')
        list_of_ensembles = truncateedit_MAX.truncate(THESEUS, cluster_path , work_dir + '/fine_cluster_1', MAX, percent, False)

        os.system('mkdir ' + work_dir + '/ensembles_1')
        for each_ens in list_of_ensembles:
            SCWRL_edit.edit_sidechains(each_ens, work_dir + '/ensembles_1/')
        sys.exit()
#--------------------------------------
# Do the clustering
#---------------------------------------
USE_SPICKER = True
if not ENSEMBLE_import:

# Spicker Alternative for clustering then MAX
#------------------------------------
    if USE_SPICKER == True:

        ResultsPath = work_dir + '/RESULTS'
        if not os.path.exists(work_dir + '/RESULTS'):
            os.mkdir(work_dir + '/RESULTS')

        # noClusters=1

        print '----- Clustering models --------\n'

        run_spicker.RUN_SPICKER(PATH_TO_MODELS, work_dir + '/spicker_run', SPICKEREXE, int(noClusters) , work_dir)
        samples = 1
        print '\nClusteing Done. using the first ' + str(samples) + ' clusters\n'
        while samples < int(noClusters) + 1:
            models_path = work_dir + '/S_clusters/cluster_' + str(samples)
            print '----- Truncating models for cluster ' + str(samples) + ' --------\n'
            if not os.path.exists(work_dir + '/fine_cluster_' + str(samples)):
                os.mkdir(work_dir + '/fine_cluster_' + str(samples))

            os.chdir(work_dir + '/fine_cluster_' + str(samples))
            #
            # Truncate models and return the path to the ensembled models - each ensemble is a single pdb file
            # containing multiple models
            if Ensembler:
                list_of_ensembles = truncateedit_MAX.truncate_Phenix(PHENIX, models_path, work_dir + '/fine_cluster_' + str(samples), MAX, percent, FIXED_INTERVALS)
            else:
                list_of_ensembles = truncateedit_MAX.truncate(THESEUS, models_path, work_dir + '/fine_cluster_' + str(samples), MAX, percent, FIXED_INTERVALS)

            if top_model_only:
                list_of_ensembles = truncateedit_MAX.One_model_only(list_of_ensembles, work_dir)

            os.system('mkdir ' + work_dir + '/ensembles_' + str(samples))
            for each_ens in list_of_ensembles:
                SCWRL_edit.edit_sidechains(each_ens, work_dir + '/ensembles_' + str(samples) + '/')

            final_ensembles = []
            for infile in glob.glob(os.path.join(work_dir + '/ensembles_' + str(samples), '*.pdb')):
                final_ensembles.append(infile)

            bump_dir = os.path.join(work_dir, 'MRBUMP')
            if not os.path.exists(bump_dir):
                os.mkdir(bump_dir)
            os.chdir(bump_dir)
            if not MISSING_DOMAINS:
                if CLUSTER:
                    sys.stdout.write("Running MR and model building on a cluster\n\n")

                    mrBuild = clusterize.ClusterRun()
                    mrBuild.QTYPE = "SGE"

                    mrBuildClusterDir = os.path.join(bump_dir, "cluster_run" + str(samples))
                    os.mkdir(mrBuildClusterDir)

                    mrBuild.HKLIN = amopt.d['mtz']
                    mrBuild.LABIN["F"] = amopt.d['F']
                    mrBuild.LABIN["SIGF"] = amopt.d['SIGF']
                    mrBuild.LABIN["FreeR_flag"] = amopt.d['FREE']
                    mrBuild.SEQIN = FASTA
                    mrBuild.getMTZInfo(mrBuild.HKLIN, mrBuildClusterDir)

                    mrBuild.BCYCLES = amopt.d['buccaneer_cycles']
                    mrBuild.BUCC = amopt.d['use_buccaneer']
                    mrBuild.SCYLCLES = amopt.d['shelx_cycles']
                    mrBuild.SHELXE = amopt.d['use_shelxe']

                    mrBuild.MRKEYS = amopt.d['mr_keys']

                    # Reset the queue list
                    mrBuild.qList = []
                    jobID = 0
                    for pdbfile in final_ensembles:
                        mrBuild.mrBuildOnCluster(mrBuildClusterDir, pdbfile, jobID, amopt.d['mrbump_programs'])
                        jobID = jobID + 1
                    mrBuild.monitorQueue()

                    shutil.rmtree(work_dir + '/fine_cluster_' + str(samples))
                    # shutil.rmtree(work_dir+'/pre_models')
                    for l in os.listdir(work_dir + '/spicker_run'):
                        if os.path.splitext(l)[1] == 'pdb':
                            os.remove(work_dir + '/spicker_run/' + l)
                    os.remove(work_dir + '/spicker_run/rep1.tra1')
                    T = printTable.Table()

                    T.bumppath = mrBuildClusterDir
                    T.cluster = True
                    table = T.maketable()
                    out = sys.stdout
                    T.pprint_table(out, table)

                    # cleanup
                # for each_run in os.listdir(mrBuildClusterDir ):
                    #   if os.path.isdir(  os.path.join(mrBuildClusterDir, each_run)):
                    #      name=re.split('_', each_run)
                    #      mrBuildOutputDir=os.path.join(bump_dir, "cluster_run"+str(samples)+"result"+name[1])
                    #      os.mkdir(mrBuildOutputDir)
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "search_"+name[1]+"_mrbump","phaser_shelx" ),mrBuildOutputDir  )
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "search_"+name[1]+"_mrbump","molrep_shelx" ),mrBuildOutputDir  )
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "search_"+name[1]+"_mrbump","data" ),mrBuildOutputDir  )
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "logs" ),mrBuildOutputDir  )
                    # shutil.rmtree(mrBuildClusterDir)

            # Monitor the cluster queue to see when all jobs have finished


                if not CLUSTER:
                    sys.stdout.write('Truncating Done for cluster ' + str(samples) + "\n\n")
                    sys.stdout.write('----- Running MRBUMP (cluster ' + str(samples) + ')--------\n\n')
                    sys.stdout.write('Created ' + str(len(final_ensembles)) + ' ensembles,   running ' + str(len(final_ensembles)) + ' jobs for cluster ' + str(samples) + '\n')

                    bump_dir = os.path.join(work_dir, 'MRBUMP_cluster' + str(samples))
                    print bump_dir

                    os.mkdir(bump_dir)
                    os.chdir(bump_dir)
                    split_ensembles = run_mr_bump_shelx_parallel.split(final_ensembles, amopt.d['nproc'])
                    run_mr_bump_shelx_parallel.split_into_runs(amopt.d['mtz'], split_ensembles, bump_dir, FASTA, amopt.d['nproc'], amopt.d['SIGF'], amopt.d['F'], amopt.d['FREE'], amopt.d['ASU'], amopt.d['early_terminate'], ResultsPath, amopt.d['use_shelxe'], amopt.d['shelx_cycles'], amopt.d['old_shelx'], amopt.d['mrbump_programs'], amopt.d['use_buccaneer'], amopt.d['buccaneer_cycles'], amopt.d['use_arpwarp'], amopt.d['arpwarp_cycles'], amopt.d['mr_keys'])

                    if amopt.d['use_shelxe']:
                        Final_display_results.make_log(bump_dir, os.path.join(work_dir, 'Final_results.log'))
                        # print '\n\nFinal Results:\n\n'
                        # T=printTable.Table()
                        # T.bumppath = work_dir +'/MRBUMP_cluster'+str(samples)
                        # T.cluster = False
                        # table = T.maketable()
                        # out = sys.stdout
                        # T.pprint_table(out, table)
                    else:
                        resultslog = open(ResultsPath + '/Results.log', "w")
                        print 'getting results from: {}',format( bump_dir )
                        for mrbumplog in os.listdir(bump_dir):
                            if re.search('.log', mrbumplog):
                                # print mrbumplog
                                for line in open(mrbumplog):
                                    if re.search('^(\d)\s*loc0_', line):
                                        if not re.search('method', line):
                                            print line
                                            resultslog.write(line)
                        resultslog.close()
                        sys.exit()

#
# --------------- missing domains cluster
#
            if MISSING_DOMAINS:
                if  CLUSTER:
                    sys.stdout.write("Running MR and model building on a cluster\n\n")

                    mrBuild = clusterize.ClusterRun()
                    mrBuild.QTYPE = "SGE"

                    mrBuildClusterDir = os.path.join(bump_dir, "cluster_run" + str(samples))
                    os.mkdir(mrBuildClusterDir)


                    mrBuild.HKLIN = amopt.d['mtz']
                    mrBuild.LABIN["F"] = amopt.d['F']
                    mrBuild.LABIN["SIGF"] = amopt.d['SIGF']
                    mrBuild.LABIN["FreeR_flag"] = amopt.d['FREE']
                    mrBuild.SEQIN = amopt.d['domain_all_chains_fasta']
                    mrBuild.getMTZInfo(mrBuild.HKLIN, mrBuildClusterDir)


                    mrBuild.BCYCLES = amopt.d['buccaneer_cycles']
                    mrBuild.BUCC = amopt.d['use_buccaneer']
                    mrBuild.SCYLCLES = amopt.d['shelx_cycles']
                    mrBuild.SHELXE = amopt.d['use_shelxe']

                    mrBuild.MRKEYS = amopt.d['mr_keys']


#            mrBuild.FIXEDIN='FIXED_XYZIN '+domain_all_chains_pdb+' IDENTIY 0.6 \n'

                    # Reset the queue list
                    mrBuild.qList = []
                    jobID = 0
                    for pdbfile in final_ensembles:
                        mrBuild.mrBuildOnCluster(mrBuildClusterDir, pdbfile, jobID , amopt.d['mrbump_programs'], amopt.d['domain_all_chains_pdb'] , '0.6')
                        jobID = jobID + 1
                    mrBuild.monitorQueue()

                    shutil.rmtree(work_dir + '/fine_cluster_' + str(samples))
                    shutil.rmtree(work_dir + '/pre_models')
                    for l in os.listdir(work_dir + '/spicker_run'):
                        if os.path.splitext(l)[1] == 'pdb':
                            os.remove(work_dir + '/spicker_run/' + l)
                    os.remove(work_dir + '/spicker_run/rep1.tra1')


                    # cleanup
                    # for each_run in os.listdir(mrBuildClusterDir ):
                    #   if os.path.isdir(  os.path.join(mrBuildClusterDir, each_run)):
                    #      name=re.split('_', each_run)
                    #      mrBuildOutputDir=os.path.join(bump_dir, "cluster_run"+str(samples)+"result"+name[1])
                    #      os.mkdir(mrBuildOutputDir)
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "search_"+name[1]+"_mrbump","phaser_shelx" ),mrBuildOutputDir  )
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "search_"+name[1]+"_mrbump","molrep_shelx" ),mrBuildOutputDir  )
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "search_"+name[1]+"_mrbump","data" ),mrBuildOutputDir  )
                    #      shutil.move (os.path.join(mrBuildClusterDir, each_run, "logs" ),mrBuildOutputDir  )
                    # shutil.rmtree(mrBuildClusterDir)

#-------------------------------------
                else:
                    # Running locally
                    #print domain_all_chains_pdb
                    #print domain_all_chain_fasta
                    bump_dir = os.path.join(work_dir, 'MRBUMP_cluster' + str(samples))
                    os.mkdir(bump_dir)
                    os.chdir(bump_dir)
                    split_ensembles = run_mr_bump_shelx_parallel.split(final_ensembles, amopt.d['nproc'])
                    run_mr_bump_shelx_parallel.split_into_runs_domains(amopt.d['mtz'], split_ensembles, bump_dir, amopt.d['domain_all_chains_fasta'] , amopt.d['nproc'], amopt.d['SIGF'], amopt.d['F'], amopt.d['FREE'], amopt.d['ASU'], amopt.d['early_terminate'], ResultsPath, amopt.d['use_shelxe'], amopt.d['shelx_cycles'], amopt.d['domain_all_chains_pdb'], FIXED_INPUT, amopt.d['old_shelx'], amopt.d['mrbump_programs'], amopt.d['use_buccaneer'], amopt.d['buccaneer_cycles'], amopt.d['use_arpwarp'], amopt.d['arpwarp_cycles'], amopt.d['mr_keys'])
                    Final_display_results.make_log(bump_dir, os.path.join(work_dir, 'Final_results.log'))


            samples += 1


        time_stop = time.time()
        elapsed_time = time_stop - time_start
        run_in_min = elapsed_time / 60
        run_in_hours = run_in_min / 60
        msg = '\nMR and shelx DONE \n ALL DONE  (in ' + str(run_in_hours) + ' hours) \n----------------------------------------\n'
        logging.info(msg)
        RUNNING.write(msg)
        RUNNING.flush()
        RUNNING.close()

        # os.system('tar -cvf '+pdb_code+'_' + work_dir+'.tar '+work_dir)
        # os.system('gzip ' +pdb_code+'_'+ work_dir+'.tar')
        # os.system('mv '+ pdb_code+'_'+  work_dir+'.tar.gz /data2/jac45 ')
        # os.system('rm '+work_dir)
        # stop here
        sys.exit()  # the above is the only code that should be used

## End not ENSEMBLE_import

#------------------------------------
# END
#----------------------------------
time_stop = time.time()

elapsed_time = time_stop - time_start
run_in_min = elapsed_time / 60
run_in_hours = run_in_min / 60

RUNNING.write('\nMODELLING and ASSEMBLY ALL DONE  (in ' + str(run_in_hours) + ' hours) \n----------------------------------------\n')
RUNNING.flush()

if ENSEMBLE_import:
    final_ensembles = []
    for infile in glob.glob(os.path.join(amopt.d['ensembles'], '*.pdb')):
        final_ensembles.append(infile)

    if top_model_only == True:
        final_ensembles = truncateedit_MAX.One_model_only(final_ensembles , work_dir)

ResultsPath = work_dir + '/RESULTS'
if not os.path.exists(work_dir + '/RESULTS'):
    os.mkdir(work_dir + '/RESULTS')

#--------------Import into MrBUMP here-------------------
RUNNING.write('Running MrBUMP\nMR LOGS in ' + work_dir + '/MRBUMP')
RUNNING.flush()

# os.system('mkdir ' + work_dir+'/MRBUMP')
amopt.d['mrbump_dir'] = os.path.join(work_dir, 'MRBUMP')
if not os.path.isdir( amopt.d['mrbump_dir'] ):
    os.mkdir(amopt.d['mrbump_dir'])
os.chdir(amopt.d['mrbump_dir'])
ResultsPath = work_dir + '/RESULTS'
if not MISSING_DOMAINS:
    if CLUSTER:
        sys.stdout.write("Running MR and model building on a cluster\n\n")

        mrBuild = clusterize.ClusterRun()
        mrBuild.QTYPE = "SGE"

        mrBuildClusterDir = os.path.join(amopt.d['mrbump_dir'], "cluster_run")
        os.mkdir(mrBuildClusterDir)

        mrBuild.HKLIN = amopt.d['mtz']
        mrBuild.LABIN["F"] = amopt.d['F']
        mrBuild.LABIN["SIGF"] = amopt.d['SIGF']
        mrBuild.LABIN["FreeR_flag"] = amopt.d['FREE']
        mrBuild.SEQIN = FASTA
        mrBuild.getMTZInfo(mrBuild.HKLIN, mrBuildClusterDir)

        mrBuild.BCYCLES = amopt.d['buccaneer_cycles']
        mrBuild.BUCC = amopt.d['use_buccaneer']
        mrBuild.SCYLCLES = amopt.d['shelx_cycles']
        mrBuild.SHELXE = amopt.d['use_shelxe']

        mrBuild.MRKEYS = amopt.d['mr_keys']

        # Reset the queue list
        mrBuild.qList = []
        jobID = 0

        if amopt.d['old_shelx']:
            mrBuild.shelxClusterScript = "python " + os.path.join(os.environ["CCP4"], "share", "ample", "python", "shelx_cluster.py")
        else:
            mrBuild.shelxClusterScript = "python " + os.path.join(os.environ["CCP4"], "share", "ample", "python", "shelxe_trace.py")

        for pdbfile in final_ensembles:
            mrBuild.mrBuildOnCluster(mrBuildClusterDir, pdbfile, jobID, amopt.d['mrbump_programs'])
            jobID = jobID + 1

        # Monitor the cluster queue to see when all jobs have finished
        mrBuild.monitorQueue()
    else:
        split_ensembles = run_mr_bump_shelx_parallel.split(final_ensembles, amopt.d['nproc'])
        run_mr_bump_shelx_parallel.split_into_runs(split_ensembles, amopt)
        Final_display_results.make_log(amopt.d['mrbump_dir'], os.path.join(work_dir, 'Final_results.log'))

        if amopt.d['use_shelxe']:
            Final_display_results.make_log(amopt.d['mrbump_dir'], os.path.join(work_dir, 'Final_results.log'))
        else:
            resultslog = open(ResultsPath + '/Results.log', "w")
            print 'getting results from:'
            print amopt.d['mrbump_dir']
            for mrbumplog in os.listdir(amopt.d['mrbump_dir']):
                if re.search('.log', mrbumplog):
                                # print mrbumplog
                    for line in open(mrbumplog):
                        if re.search('^(\d)\s*loc0_', line):
                            if not re.search('method', line):
                                print line
                                resultslog.write(line)
            resultslog.close()
            sys.exit()

if MISSING_DOMAINS:
    if  CLUSTER:

        sys.stdout.write("Running MR and model building on a cluster\n\n")

        mrBuild = clusterize.ClusterRun()
        mrBuild.QTYPE = "SGE"

        mrBuildClusterDir = os.path.join(amopt.d['mrbump_dir'], "cluster_run")
        os.mkdir(mrBuildClusterDir)

        mrBuild.HKLIN = amopt.d['mtz']
        mrBuild.LABIN["F"] = amopt.d['F']
        mrBuild.LABIN["SIGF"] = amopt.d['SIGF']
        mrBuild.LABIN["FreeR_flag"] = amopt.d['FREE']
        mrBuild.SEQIN = str(amopt.d['domain_all_chains_fasta'])
        mrBuild.getMTZInfo(mrBuild.HKLIN, mrBuildClusterDir)

        mrBuild.BCYCLES = amopt.d['buccaneer_cycles']
        mrBuild.BUCC = amopt.d['use_buccaneer']
        mrBuild.SCYLCLES = amopt.d['shelx_cycles']
        mrBuild.SHELXE = amopt.d['use_shelxe']

        mrBuild.MRKEYS = amopt.d['mr_keys']
        print amopt.d['mr_keys']
        if amopt.d['old_shelx']:
            mrBuild.shelxClusterScript = "python " + os.path.join(os.environ["CCP4"], "share", "ample", "python", "shelx_cluster.py")
        else:
            mrBuild.shelxClusterScript = "python " + os.path.join(os.environ["CCP4"], "share", "ample", "python", "shelxe_trace.py")

        # Reset the queue list
        mrBuild.qList = []
        jobID = 0
        for pdbfile in final_ensembles:
            mrBuild.mrBuildOnCluster(mrBuildClusterDir, pdbfile, jobID, amopt.d['mrbump_programs'], amopt.d['domain_all_chains_pdb'], 0.6)
            jobID = jobID + 1

    else:
        print amopt.d['domain_all_chains_pdb']
        print amopt.d['domain_all_chains_fasta']
        print 'the input is fixed ', FIXED_INPUT
        split_ensembles = run_mr_bump_shelx_parallel.split(final_ensembles, amopt.d['nproc'])
        run_mr_bump_shelx_parallel.split_into_runs_domains(amopt.d['mtz'], split_ensembles, amopt.d['mrbump_dir'], amopt.d['domain_all_chains_fasta'] , amopt.d['nproc'], amopt.d['SIGF'], amopt.d['F'], amopt.d['FREE'], amopt.d['ASU'], amopt.d['early_terminate'], ResultsPath, amopt.d['use_shelxe'], amopt.d['shelx_cycles'], amopt.d['domain_all_chains_pdb'], FIXED_INPUT, amopt.d['old_shelx'], amopt.d['mrbump_programs'], amopt.d['use_buccaneer'], amopt.d['buccaneer_cycles'], amopt.d['use_arpwarp'], amopt.d['arpwarp_cycles'], amopt.d['mr_keys'])
        Final_display_results.make_log(amopt.d['mrbump_dir'], os.path.join(work_dir, 'Final_results.log'))


time_stop = time.time()
elapsed_time = time_stop - time_start
run_in_min = elapsed_time / 60
run_in_hours = run_in_min / 60

RUNNING.write('\nMR and shelx ALL DONE  (in ' + str(run_in_hours) + ' hours) \n----------------------------------------\n')
RUNNING.flush()
