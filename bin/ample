#!/usr/bin/env python

"""
Version 0.1
no speed buffs, no extra functionality, no optimisation
Basic Pipeline
for multi core desktops, not clusters

Issues:
1) if this script is killed, the rosetta will continue
2) Theseus can hang, this is killed after a timeout
3) is clustered by all atom RMSD to make fine clusters (option to cluster by CA only i included)
4) ASU content is the number of search models placed by MrBUMP. -- need to set this manually


================================

so...
Only data passed between different stages are the pdb files, so each stage could just start from a directory of files

"""

import os
import sys

# Test for environment variables

if not "CCP4" in sorted(os.environ.keys()):
    raise RuntimeError('CCP4 not found')

# Add the ample python folder to the PYTHONPATH
sys.path.append(os.path.join(os.environ["CCP4"], "share", "ample", "python"))

# python imports
import argparse
import clusterize
import glob
import logging
import re
import shutil
import time

# Our imports
import ample_options
import ample_util
import fasta_parser
import mrbump_ensemble
import nmr
import rosetta_model
import mrbump_results
import run_spicker
import SCWRL_edit
import truncateedit_MAX


#-------------------------------------------
# get command line options
#-------------------------------------------

parser = argparse.ArgumentParser(description='Structure solution by abinitio modelling', prefix_chars="-")

parser.add_argument('-alignment_file', metavar='alignment_file', type=str, nargs=1,
                   help='alignment between homolog and target fastas')

parser.add_argument('-allatom', metavar='allatom', type=str, nargs=1,
                   help='allatom')

parser.add_argument('-arpwarp_cycles', metavar='arpwarp_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-ASU', metavar='no in ASU', type=int, nargs=1,
                   help='no in ASU')

parser.add_argument('-blast_dir', metavar='blast_dir', type=str, nargs=1,
                   help='Directory where ncbi blast is installed (binaries in bin)')

parser.add_argument('-buccaneer_cycles', metavar='buccaneer_cycles', type=int, nargs=1,
                   help='number of cycles ')

parser.add_argument('-CC', metavar='rad of gyration', type=str, nargs=1,
                   help='radius of gyration reweight ')

parser.add_argument('-submit_cluster', metavar='submit_cluster', type=str, nargs=1,
                   help='will submit jobs to a cluster')

parser.add_argument('-submit_qtype', metavar='submit_qtype', type=str, nargs=1,
                   help='cluster submission queue type:SGE, LSF...')

parser.add_argument('-debug', metavar='debug option', type=str, nargs=1,
                   help='debug option ')

parser.add_argument('-domain_all_chains_pdb', metavar='domain_all_chains_pdb', type=str, nargs=1,
                   help='fixed input to mr bump')

parser.add_argument('-domain_termini_distance', metavar='termini', type=str, nargs=1,
                   help='distance between termini for insert domains')

parser.add_argument('-early_terminate', metavar='early_terminate', type=str, nargs=1,
                     help='terminate early if a success')

parser.add_argument('-ensembler', metavar='ensembler', type=str, nargs=1,
                   help='ensembling')

parser.add_argument('-ensembles', metavar='enembles', type=str, nargs=1,
                   help='path to ensembles')

parser.add_argument('-fasta', metavar='fasta_file', type=str, nargs=1,
                   help='protein fasta file. (required)')

parser.add_argument('-F', metavar='flag for F', type=str, nargs=1,
                   help='Flag for F')

parser.add_argument('-frags3mers', metavar='frags3mers', type=str, nargs=1,
                   help='path of imported 3mers')

parser.add_argument('-frags9mers', metavar='frags9mers', type=str, nargs=1,
                   help='path of imported 9mers')

parser.add_argument('-FREE', metavar='flag for FREE', type=str, nargs=1,
                   help='Flag for FREE')

parser.add_argument('-import_cluster', metavar='import_cluster', type=str, nargs=1,
                   help='import_cluster')

parser.add_argument('-improve_template', metavar='improve template', type=str, nargs=1,
                   help='give a template to imrove - NMR, homolog ')

parser.add_argument('-LGA', metavar='path_to_LGA dir', type=str, nargs=1,
                   help='pathway to LGA folder (not the exe) will use the \'lga\' executable')

parser.add_argument('-make_frags', metavar='bool to make fragments', type=str, nargs=1,
                   help='Bool, True to make non homologous framents, False to import fragments')

parser.add_argument('-make_models', metavar='Do the modelling', type=str, nargs=1,
                   help='run rosetta modeling, set to False to import pre-made models (required if making models locally default True)')

parser.add_argument('-maxcluster_exe', metavar='Maxcluster exe', type=str, nargs=1,
                   help='Maxcluster exe')

parser.add_argument('-missing_domain', metavar='missing_domain', type=str, nargs=1,
                   help='Modelling a missing domain - requires domain_all_chains_pdb argument')

parser.add_argument('-models_dir', metavar='folder of decoys', type=str, nargs=1,
                   help='folder of decoys')

parser.add_argument('-molreponly', metavar='molreponly', type=str, nargs=1,
                   help='molreponly')

parser.add_argument('-mr_keys', metavar='-mr_keys', type=str, nargs=1,
                   help='mrbump keywords ')

parser.add_argument('-mtz', metavar='MTZ in', type=str, nargs=1,
                   help='MTZ in')

parser.add_argument('-name', metavar='priotein name', type=str, nargs=1,
                   help='name of protien in the format ABCD ')

parser.add_argument('-nmodels', metavar='no models', type=int, nargs=1,
                   help='number of models to make (1000)')

parser.add_argument('-nr', metavar='nr', type=str, nargs=1,
                   help='Path to the nr non-redundant sequence database')

parser.add_argument('-old_shelx', metavar='old_shelx', type=str, nargs=1,
                   help='old_shelx')

parser.add_argument('-NMR_model_in', metavar='NMR_model_in', type=str, nargs=1,
                   help='use nmr input')

parser.add_argument('-NMR_process', metavar='NMR_process', type=str, nargs=1,
                   help='number of times to process the models')

parser.add_argument('-NMR_remodel_fasta', metavar='-NMR_remodel_fasta', type=str, nargs=1,
                   help='fasta_for_remodeling')

parser.add_argument('-NMR_Truncate_only', metavar='-NMR_Truncate_only', type=str, nargs=1,
                   help='do no remodelling only truncate the NMR')

parser.add_argument('-nproc', metavar='NoProcessors', type=int, nargs=1,
                   help='number of processers (default 1)')

parser.add_argument('-num_clusters', metavar='num clusters to sample', type=int, nargs=1,
                   help='number of clusters to sample')

parser.add_argument('-percent', metavar='no clus to sample', type=str, nargs=1,
                   help='percent interval for truncation')

parser.add_argument('-phaseronly', metavar='phaseronly', type=str, nargs=1,
                   help='phaseronly')

parser.add_argument('-phenix_exe', metavar='PHENIX.ensembler exe', type=str, nargs=1,
                   help='path to phenix executable')


parser.add_argument('-ROSETTA', metavar='ROSETTA_path', type=str, nargs=1,
                   help='path for Rosetta AbinitioRelax')

parser.add_argument('-ROSETTA_cluster', metavar='path to Rosettas cluster', type=str, nargs=1,
                   help='location of rosetta cluster')

parser.add_argument('-rosetta_db', metavar='ROSETTA_database', type=str, nargs=1,
                   help='path for Rosetta database')

parser.add_argument('-rosetta_dir', metavar='Rosetta_dir', type=str, nargs=1,
                   help='the Rosetta install directory')

parser.add_argument('-rosetta_fragments_exe', metavar='path to make_fragments.pl', type=str, nargs=1,
                   help='location of make_fragments.pl')

parser.add_argument('-run_dir', metavar='run_directory', type=str, nargs=1,
                   help='directory to put files (default current dir)')

parser.add_argument('-scwrl_exe', metavar='path to scwrl', type=str, nargs=1,
                   help='pathway to SCWRL exe')

parser.add_argument('-shelx_cycles', metavar='shelx_cycles', type=str, nargs=1,
                     help='number of shelx sycles')

parser.add_argument('-shelxe_exe', metavar='-shelxe_exe', type=str, nargs=1,
                   help='Set shelxe executable name (default "shelxe")')

parser.add_argument('-SIGF', metavar='flag for SIGF', type=str, nargs=1,
                   help='Flag for SIGF')

parser.add_argument('-spicker_exe', metavar='boolean', type=str, nargs=1,
                   help='path to spicker executable')

parser.add_argument('-theseus_exe', metavar='Theseus exe (required)', type=str, nargs=1,
                   help='path to theseus executable')

parser.add_argument('-top_model_only', metavar='top_model_only', type=str, nargs=1,
                   help='top_model_only')

parser.add_argument('-transmembrane', metavar='transmembrane', type=str, nargs=1,
                   help='Do modelling for transmembrane proteins')

parser.add_argument('-transmembrane_spanfile', metavar='transmembrane_spanfile', type=str, nargs=1,
                   help='span file for modelling transmembrane proteins')

parser.add_argument('-transmembrane_lipofile', metavar='transmembrane_lipofile', type=str, nargs=1,
                   help='lips4 file for modelling transmembrane proteins')

parser.add_argument('-usehoms', metavar='nohoms rosetta flag', type=str, nargs=1,
                   help='True =use nhomologs, False= dont use them ')

parser.add_argument('-use_arpwarp', metavar='-use_arpwarp', type=str, nargs=1,
                   help='True to use arpwarp ')

parser.add_argument('-use_buccaneer', metavar='use_buccaneer', type=str, nargs=1,
                   help='True to use Buccaneer')

parser.add_argument('-use_scwrl', metavar='USE_SCWRL', type=str, nargs=1,
                   help='if using scwrl true if not false')

parser.add_argument('-use_shelxe', metavar='-use_shelxe', type=str, nargs=1,
                   help='True to use shelx ')

#
# jmht - set the default values here
#
# name -> pdb_code
# SCRWL -> SCWRL_path
# maxcluster_exe -> maxcluster_exe_path
parser.set_defaults( 
                    alignment_file = '',
                    allatom = "true",
                    arpwarp_cycles = 10,
                    ASU = 0,
                    buccaneer_cycles = 5,
                    CC = '',
                    import_cluster = "false",
                    debug = "false",
                    domain_all_chains_pdb = None,
                    domain_termini_distance = 0,
                    ensembles = None,
                    ensembler = "false",
                    fasta = None,
                    frags3mers=None,
                    frags9mers=None,
                    improve_template = None,
                    maxcluster_exe = None,
                    make_frags = "true",
                    make_models = "true",
                    missing_domain = "false",
                    models_dir = None,
                    molreponly = "false",
                    mtz = None,
                    name = None,
                    num_clusters = 1,
                    nmodels = 1000,
                    NMR_model_in = None,
                    NMR_process = None,
                    NMR_remodel_fasta = None,
                    NMR_Truncate_only = "false",
                    nproc = 1,
                    shelx_cycles = 15,
                    old_shelx = "false",
                    percent = 5,
                    phaseronly = "false",
                    phenix_exe = None,
                    rosetta_cluster = None,
                    rosetta_db  = None,
                    rosetta_fragments_exe = None,
                    rosetta_path =  None,
                    run_dir = os.getcwd(),
                    scwrl_exe = None,
                    spicker_exe = None,
                    submit_cluster = "false",
                    submit_qtype = "SGE",
                    usehoms = "true",
                    use_arpwarp = "true",
                    use_buccaneer = "true",
                    use_scwrl = 'false',
                    use_shelxe = "false",
                    shelxe_exe = "shelxe",
                    top_model_only = "false",
                    theseus_exe = None,
                    early_terminate = "true"
                    )


# Create amopt object to hold options
amopt = ample_options.AmpleOptions( )

# MRkeys hack - get MRBUMP keywords direct as there can be multiple arguments
# to each one
MRkeys = []
#print sys.argv
keycount = 0
toRemove = [] # We remove all the mrkeywords
while keycount < len(sys.argv):
    #print sys.argv[keycount] ,  keycount
    if sys.argv[keycount] == "-mr_keys":
        toRemove.append(keycount)
        keycount += 1
        while not sys.argv[keycount].startswith("-"):
            if keycount > len(sys.argv):
                break
            MRkeys.append( sys.argv[keycount] )
            toRemove.append( keycount )
            keycount += 1
    keycount += 1

# Need to decrement the index of the items to remove
# as we remove them
for count, i in enumerate(toRemove):
    del sys.argv[i-count]
## End MRKeys hack..

# convert args to dictionary
args = parser.parse_args()

# Now put them in the amopt object
amopt.populate( args )

# Now set MRKeys
amopt.d['mr_keys'] = MRkeys

# qtype always use uppercase
amopt.d['submit_qtype'] = amopt.d['submit_qtype'].upper()

# Make a work directory and go there - this way all output goes into this directory
if not os.path.exists(amopt.d['run_dir']):
    print 'You need to give a run directory'
    sys.exit()

print 'Making a Run Directory  -checking for previous runs\n'
amopt.d['work_dir'] = ample_util.make_workdir( amopt.d['run_dir'] )
os.chdir( amopt.d['work_dir'] )

# Set up logging
logger = ample_util.setup_logging()

#jmht - fix these
FIXED_INPUT = True
pdb_code = amopt.d['name']
amopt.d['pdb_code'] = amopt.d['name']

if not amopt.d['pdb_code'] or len(pdb_code) != 4:
    msg = 'Name is the wrong length, use 4 chars eg ABCD'
    logger.critical(msg)
    raise RuntimeError, msg
else:
    amopt.d['pdb_code'] += '_'
    
# Check we can find the input fasta
if not ( os.path.exists(str(amopt.d['fasta'])) or os.path.exists(str(amopt.d['NMR_remodel_fasta'])) ):
    msg = 'You need to give the path for the fasta'
    logger.critical(msg)
    sys.exit(1)

# Reformat to what we need    
logger.debug('Parsing FASTA file')
#jmht - for TM test case - not sure if we should leave this in
amopt.d['orig_fasta'] = amopt.d['fasta']
outfasta = os.path.join( amopt.d['work_dir'], amopt.d['pdb_code'] + '_.fasta')
fasta_parser.parse_fasta( amopt.d['fasta'], outfasta )
amopt.d['fasta'] = outfasta
    
# Check if importing ensembles
amopt.d['import_ensembles'] = False
if amopt.d['ensembles']:
    if not os.path.isdir( amopt.d['ensembles'] ):
        msg = "Trying to import ensembles from the directory:\n{0}\nBut it is not a directory!".format(amopt.d['ensembles'])
        logger.critical(msg)
        sys.exit(1)

    amopt.d['import_ensembles'] = True
    logger.info("Found directory with ensemble files: {0}\nSetting make_models to False".format( amopt.d['ensembles'] ) )
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False
    
# Check if importing models
amopt.d['import_models'] = False
if amopt.d['models_dir']:
    if not os.path.isdir( amopt.d['models_dir'] ):
        msg = "Trying to import models from the directory:\n{0}\nBut it is not a directory!".format(amopt.d['models_dir'])
        logger.critical(msg)
        sys.exit(1)
    
    amopt.d['import_models'] = True
    amopt.d['make_frags'] = False
    amopt.d['make_models'] = False
else:
    # Create the models_dir
    amopt.d['models_dir'] = amopt.d['work_dir'] + os.sep + "models"
    os.mkdir( amopt.d['models_dir'] ) 

# Check models and ensembles
if amopt.d['import_ensembles'] and amopt.d['import_models']:
        msg = "Cannot import both models and ensembles!"
        logger.critical(msg)
        sys.exit(1)

if amopt.d['make_frags'] and (  amopt.d['frags3mers'] or  amopt.d['frags9mers'] ):
    msg = "make_frags set to true, but you have given the path to the frags3mers or frags9mers"
    logger.critical(msg)
    sys.exit(1)


# NMR Checks
NMR_PROTOCOL = False
if amopt.d['NMR_model_in']:
    
    if not os.path.isfile( amopt.d['NMR_model_in'] ):
        msg = "NMR_model_in flag given, but cannot find file: {0}".format( amopt.d['NMR_model_in'] )
        logger.critical(msg)
        sys.exit(1)    
       
    NMR_PROTOCOL = True
    ROSETTA_CM = amopt.d['rosetta_dir'] + '/rosetta_source/bin/mr_protocols.default.linuxgccrelease'
    if not os.path.exists(ROSETTA_CM) :
        msg = 'Cannot find {0} check path names'.format( ROSETTA_CM )
        logger.critical( msg )
        sys.exit(1)

    if not os.path.isfile( str(amopt.d['NMR_remodel_fasta']) ):
        amopt.d['NMR_remodel_fasta'] =  amopt.d['fasta']

if not  NMR_PROTOCOL :
    if amopt.d['import_models'] and not os.path.exists(amopt.d['models_dir']):
        logger.warn('you have chosen to import models, but path does not exist, looking for ensembles : ' + amopt.d['models_dir'])
        if not os.path.exists(amopt.d['ensembles']):
            logger.warn('you have chosen to import ensembles, but path does not exist : ' + amopt.d['ensembles'])
            if amopt.d['import_cluster']:
                logger.info('you are importing a cluster of models from :' + amopt.d['import_cluster'])
                if not os.path.exists(amopt.d['import_cluster']):
                    logger.critical('you have chosen to import a cluster, the clusterpath does not exist')
                    sys.exit(1)

# Missing domains
if amopt.d['missing_domain']:
    logger.info('Processing missing domain\n')
    if not os.path.exists( amopt.d['domain_all_chains_pdb'] ):
        msg = 'Cannot find file domain_all_chains_pdb: {0}'.format( amopt.d['domain_all_chains_pdb'] )
        logger.critical( msg )
        sys.exit(1)


# Create the rosetta modeller - this runs all the checks required
if amopt.d['make_models'] or amopt.d['make_frags']:  # only need Rosetta if making models
    logger.info('Making models or fragments so checking ROSETTA options')
    rosetta_modeller = rosetta_model.RosettaModel()
    rosetta_modeller.set_from_dict( amopt.d )

amopt.d['mrbump_programs'] = ' molrep phaser '
if not amopt.d['molreponly'] and not amopt.d['phaseronly']:
    amopt.d['mrbump_programs'] = ' molrep phaser '
if amopt.d['molreponly']:
    amopt.d['mrbump_programs'] = ' molrep  '
if amopt.d['phaseronly']:
    amopt.d['mrbump_programs'] = ' phaser  '
if amopt.d['molreponly'] and amopt.d['phaseronly']:
        logger.critical('you say you want molrep only AND phaser only, choose one or both')
        sys.exit(1)    

#
#Check we can find all the required programs
#
if amopt.d['ensembler']:
    logger.info('You are using Phenix ensembler')
    amopt.d['phenix_exe'] = ample_util.check_for_exe('phenix', amopt.d['phenix_exe'])
else:
    amopt.d['theseus_exe'] = ample_util.check_for_exe('theseus', amopt.d['theseus_exe'])

if amopt.d['use_scwrl']:
    amopt.d['scwrl_exe'] = ample_util.check_for_exe('Scwrl4', amopt.d['scwrl_exe'] )

amopt.d['maxcluster_exe'] = ample_util.check_for_exe('maxcluster', amopt.d['maxcluster_exe'])
amopt.d['spicker_exe'] = ample_util.check_for_exe('spicker', amopt.d['spicker_exe'])

# Check if we need to get any of the flags from the MTZ file
if not os.path.isfile( amopt.d['mtz'] ):
    logger.critical("Cannot find MTZ file: {0}".format( amopt.d['mtz'] ) )
    sys.exit(1)

if not amopt.d['F'] or not amopt.d['SIGF'] or not amopt.d['FREE']:
    
    logger.info("Determining MTZ flags using mtzdump from file: {0}".format( amopt.d['mtz'] ) )
    try:
        t_flag_F, t_flag_SIGF, t_flag_FREE = ample_util.get_mtz_flags( amopt.d['mtz'] )
    except KeyError,e:
        logger.critical("Error generating flags from MTZ file: {0}\n{1}\nYou may need to run the CCP4 uniqueify on the MTZ file".format(amopt.d['mtz'],e) )
        sys.exit(1)
    
if not amopt.d['F']:
    amopt.d['F'] = t_flag_F
    
if not amopt.d['SIGF']:
    amopt.d['SIGF'] = t_flag_SIGF
    
if not amopt.d['FREE']:
    amopt.d['FREE'] = t_flag_FREE


# Print out what is being done
if amopt.d['use_buccaneer']:
    logger.info('Rebuilding in Bucaneer')
else:
    logger.info('Not rebuilding in Bucaneer')

if amopt.d['use_arpwarp']:
    logger.info('Rebuilding in ARP/wARP')
else:
    logger.info('Not rebuilding in ARP/wARP')

if amopt.d['make_frags']:
    logger.info('Making non homologusFragments')
else:
    logger.info('NOT Making Fragments')

if amopt.d['make_models']:
    logger.info('\nMaking Rosetta Models\n')
else:
    logger.info('NOT Making Rosetta Models\n')

logger.info('All needed programs are found, continuing Run')

#----------------------------
# Running is the 'official' output file 
#----------------------------
RUNNING = open( amopt.d['work_dir'] + '/ROSETTA.log', "w")
RUNNING.write(ample_util.header)
RUNNING.flush()

#----------------------------
# params used
#---------------------------
Run_params = open( amopt.d['work_dir'] + '/Params_used', "w")
param_str = amopt.prettify_parameters()
Run_params.write( param_str )
Run_params.close()
# Echo to log too
logger.debug( param_str )

#######################################################
#
# SCRIPT PROPER STARTS HERE
#
######################################################
time_start = time.time()

#-----------------------------------
# Do The Modelling
#-----------------------------------

#-----------------------------------
# Make Rosetta fragments
#-----------------------------------
if amopt.d['make_frags']:
    rosetta_modeller.generate_fragments()

#-----------------------------------
# break here for NMR (frags needed but not modelling
# if NMR process models first
#-----------------------------------
if NMR_PROTOCOL:
    nmr.doNMR(amopt, logger)
# return from nmr with models already made

#-----------------------------------
# Make the models
#-----------------------------------
if amopt.d['make_models']:
    
    logger.info('----- making Rosetta models--------\n')
    logger.info('making ' + str(amopt.d['nmodels']) + ' models...')
    RUNNING.write('\n----- making models--------\n')
    RUNNING.flush()
    
    # If we are running with cluster support submit all modelling jobs to the cluster queue
    if amopt.d['submit_cluster']:
        
        # Generate the list of random seeds
        rosetta_modeller.generate_seeds( amopt.d['nmodels'] )

        # Invoke the cluster run class
        cluster_run = clusterize.ClusterRun()
        cluster_run.QTYPE = amopt.d['submit_qtype']
        cluster_run.setModeller( rosetta_modeller )
        cluster_run.setupModellingDir( amopt.d['work_dir'] )
        
        logger.info('Submitting {0} jobs to a queueing system of type: {1}\n'.format( amopt.d['nmodels'],  amopt.d['submit_qtype'] ) )
        # loop over the number of models and submit a job to the cluster
        for i in range( amopt.d['nmodels'] ):
            nproc=1 # each modelling job runs on a single processor
            jobNumber=i+1
            cluster_run.modelOnCluster( nproc, jobNumber )

        # Monitor the cluster queue to see when all jobs have finished
        cluster_run.monitorQueue()

    else:
        # Run locally
        amopt.d['models_dir'] = rosetta_modeller.doModelling()
        
    ##End IF amopt.d['submit_cluster']
    
    msg = '\nModelling complete - models stored in:\n   ' + amopt.d['models_dir'] + '\n\n'
else:
    msg = '\nImporting models from directory:\n   ' + amopt.d['models_dir'] + '\n\n'
    
RUNNING.write(msg)
logger.info(msg)
    

#--------------------------------------------
# check if models are present regardless
#--------------------------------------------
if amopt.d['import_cluster']:
    if os.path.exists(amopt.d['import_cluster']): 
        cluster_path = amopt.d['import_cluster']
        fcluster_dir = os.path.join( amopt.d['work_dir'], 'fine_cluster_1' )
        if not os.path.exists( fcluster_dir  ):
            os.mkdir( fcluster_dir )
        os.chdir( fcluster_dir )
        list_of_ensembles = truncateedit_MAX.truncate( amopt.d['theseus_exe'],
                                                       cluster_path ,
                                                       fcluster_dir ,
                                                       amopt.d['maxcluster_exe'], 
                                                       amopt.d['percent'] )

        os.system('mkdir ' + amopt.d['work_dir'] + '/ensembles_1')
        for each_ens in list_of_ensembles:
            SCWRL_edit.edit_sidechains(each_ens, amopt.d['work_dir'] + '/ensembles_1/')
        sys.exit()

#---------------------------------------
# Do the clustering
#---------------------------------------
ensembles = [] # List of ensembles - 1 per cluster

if amopt.d['import_ensembles']:
    #---------------------------------------
    # Importing pre-made ensembles
    #---------------------------------------
    # Set list of ensembles to the one we are importing
    logger.info("Importing existing ensembles from: {0}".format( amopt.d['ensembles'] ) )
    final_ensembles = []
    for infile in glob.glob(os.path.join(amopt.d['ensembles'], '*.pdb')):
        final_ensembles.append(infile)

    if amopt.d['top_model_only']:
        logger.info("Only using the top model" )
        final_ensembles = truncateedit_MAX.One_model_only(final_ensembles , amopt.d['work_dir'])
        
    # Set ensembles
    ensembles = [ final_ensembles ]

else:

    #---------------------------------------
    # Generating  ensembles
    #---------------------------------------
    logger.info('----- Clustering models --------')
    
    # Spicker Alternative for clustering then MAX
    run_spicker.RUN_SPICKER(amopt.d['models_dir'], amopt.d['work_dir'] + '/spicker_run', amopt.d['spicker_exe'], amopt.d['num_clusters'] , amopt.d['work_dir'])
    
    # Generate list of clusters to sample
    # Done like this so it's easy to run specific clusters
    cluster_nums = [ i for i in range(1,amopt.d['num_clusters']+1)]
    
    logger.info('\nClustering Done. Using the following clusters: {0}'.format(cluster_nums) )
    
    for cluster in cluster_nums:
        
        logger.info('----- Truncating models for cluster {0} --------'.format(cluster)   )
        
        models_path = os.path.join( amopt.d['work_dir'], 'S_clusters', 'cluster_{0}'.format(cluster) )
        fcluster_dir =  os.path.join( amopt.d['work_dir'], 'fine_cluster_{0}'.format(cluster) )
        if not os.path.exists( fcluster_dir ):
            os.mkdir( fcluster_dir )
        os.chdir( fcluster_dir )
        
        logger.info("Truncating models in directory: {0}".format( fcluster_dir ) )
        
        # Truncate models and return the path to the ensembled models - each ensemble is a single pdb file
        # containing multiple models
        if amopt.d['ensembler']:
            list_of_ensembles = truncateedit_MAX.truncate_Phenix( amopt.d['phenix_exe'],
                                                                  models_path,
                                                                  fcluster_dir,
                                                                  amopt.d['maxcluster_exe'], 
                                                                  amopt.d['percent'] )
        elif amopt.d['top_model_only']:
            list_of_ensembles = truncateedit_MAX.One_model_only( list_of_ensembles, amopt.d['work_dir'] )
        else:
            list_of_ensembles = truncateedit_MAX.truncate( amopt.d['theseus_exe'],
                                                           models_path,
                                                           fcluster_dir,
                                                           amopt.d['maxcluster_exe'],
                                                           amopt.d['percent'] )
    
    
        ensemble_dir = os.path.join( amopt.d['work_dir'], 'ensembles_{0}'.format(cluster) )
        os.mkdir( ensemble_dir )
        for each_ens in list_of_ensembles:
            SCWRL_edit.edit_sidechains(each_ens, ensemble_dir + '/')
    
        final_ensembles = []
        for infile in glob.glob( os.path.join( ensemble_dir, '*.pdb' ) ):
            final_ensembles.append(infile)
            
        # Add to the total list
        ensembles.append(final_ensembles)
        
        logger.info("Truncating Done for cluster {0}".format( cluster ) )
        logger.info('Created {0} ensembles'.format( len(final_ensembles) ) )
 

#---------------------------------------
# MR BUMP analysis of the ensembles
#---------------------------------------
bump_dir = os.path.join(amopt.d['work_dir'], 'MRBUMP')
logger.info("Running MRBUMP jobs in directory: {0}".format( bump_dir ) )
if not os.path.exists(bump_dir):
    os.mkdir(bump_dir)
os.chdir(bump_dir)
amopt.d['mrbump_dir'] = bump_dir

# Loop over clusters
for i, ensemble in enumerate(ensembles):
    
    clusterID=str(i+1)
    logger.info('----- Running MRBUMP (cluster ' + clusterID+ ')--------\n\n')
    logger.info("Running {0} MRBUMP jobs for cluster: {1}".format( len(ensemble),  clusterID ) )
    
    job_dir = os.path.join( amopt.d['mrbump_dir'], 'cluster_'+clusterID )

    logger.info("Running cluster {0} in directory: {1}".format( clusterID, job_dir ) )
    if not os.path.exists( job_dir ):
        os.mkdir( job_dir )
    os.chdir( job_dir )
    
    if amopt.d['submit_cluster']:
        mrbump_ensemble.mrbump_ensemble_cluster( ensemble, job_dir, amopt.d, clusterID=clusterID )
    else:
        mrbump_ensemble.mrbump_ensemble_local( ensemble, amopt.d, clusterID=clusterID )
        
    # Show what happened
    resultsSummary = mrbump_results.ResultsSummary()   
    
    if amopt.d['submit_cluster']:    
        summary = resultsSummary.summariseResults( job_dir, cluster=True )
    else:
        summary = resultsSummary.summariseResults( job_dir, cluster=False )
        
    logger.info( summary )
    
# Timing data
time_stop = time.time()
elapsed_time = time_stop - time_start
run_in_min = elapsed_time / 60
run_in_hours = run_in_min / 60
msg = '\nMR and shelx DONE\n\n ALL DONE  (in ' + str(run_in_hours) + ' hours) \n----------------------------------------\n'
logging.info(msg)
RUNNING.write(msg)
RUNNING.flush()

# os.system('tar -cvf '+pdb_code+'_' + work_dir+'.tar '+work_dir)
# os.system('gzip ' +pdb_code+'_'+ work_dir+'.tar')
# os.system('mv '+ pdb_code+'_'+  work_dir+'.tar.gz /data2/jac45 ')
# os.system('rm '+work_dir)
sys.exit(0)
#------------------------------------
# END
#----------------------------------
